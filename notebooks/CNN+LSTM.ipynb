{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZRsyM+mOUBsS8u6eP6bSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivyson/Neural-Network-XOR/blob/main/notebooks/CNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN + LSTM\n",
        "\n",
        "Now we are working on combining two architectures we've already designed: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). LSTM handles **temporal** data, while CNN handles **spatial or positional** data. With this in mind, we can make a new architecture that processes data that is both spatial and temporalâ€”like video.\n",
        "\n",
        "A video is basically a sequence of images, called **frames**. If we feed a single frame to a CNN, it will classify that frame on its own. But for video, we don't want to just look at one frame; we want to consider multiple frames together. For example, recognizing if a person is dancing or making a specific hand gesture is hard from one frame alone. We need to see how their stance changes over time.\n",
        "\n",
        "Here, instead of using one frame, we take **n frames** and process them together. Each frame goes through a CNN to produce a **feature vector**, which captures spatial information like edges, textures, and objects. Then, the sequence of feature vectors is fed into an LSTM, which looks at how things change over time.\n",
        "\n",
        "The **order matters**: CNN first, then LSTM. If we start with LSTM, we'd lose the spatial information from the frames, and the model wouldn't work well.\n",
        "\n",
        "So the pipeline looks like this:\n",
        "\n",
        "**Frames $\\rightarrow$ CNN $\\rightarrow$ Feature vectors $\\rightarrow$ LSTM $\\rightarrow$ Output**\n",
        "\n",
        "In this notebook, we won't go deep into math since that's already covered in previous notebooks. The focus is just on combining the two architectures, and any extra workarounds will be explained as needed.\n",
        "\n",
        "The Architecture will look as follows:\n",
        "![](https://www.researchgate.net/publication/364039225/figure/fig4/AS:11431281414868010@1746027141215/CNN-LSTM-architecture.tif)\n",
        "\n",
        "\n",
        "Coding Time. Remember the philosophy, build it from scatch.\n",
        "\n",
        "Now, Before we start, Remember\n",
        "\n",
        "For each time step $( t )$:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f_t &= \\sigma \\left(W_f \\cdot \\left[h_{t-1}, x_t \\right] + b_f \\right) \\quad \\text{(forget gate)} \\\\\n",
        "i_t &= \\sigma \\left(W_i \\cdot [h_{t-1}, x_t] + b_i \\right) \\quad \\text{(input gate)} \\\\\n",
        "\\tilde{c_t} &= \\tanh \\left(W_c \\cdot \\left[h_{t-1}, x_t \\right] + b_c \\right) \\quad \\text{(candidate cell)} \\\\\n",
        "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c_t} \\quad \\text{(cell state)} \\\\\n",
        "o_t &= \\sigma \\left(W_o \\cdot [h_{t-1}, x_t] + b_o \\right) \\quad \\text{(output gate)} \\\\\n",
        "h_t &= o_t \\odot \\tanh(c_t) \\quad \\text{(hidden state)}\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "GdHGrdQ3YB3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfAckoZVYAL0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Optional, List, Dict, Any, Callable, Union\n",
        "import pickle\n",
        "import scipy # Refrain from using the convolution from scipy, computationalyy expensive for video processing, Explore the usage of im2col for Fast-Fourier Convolution Methods.\n",
        "\n",
        "ADAM_BETA1 = 0.9\n",
        "ADAM_BETA2 = 0.999\n",
        "ADAM_EPSILON = 1e-8\n",
        "LEAKY_RELU_ALPHA = 0.01\n",
        "\n",
        "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculates Mean Squared Error loss.\"\"\"\n",
        "    return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n",
        "\n",
        "def mean_squared_error_gradient(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculates the gradient of Mean Squared Error loss.\"\"\"\n",
        "    return 2 * (y_pred - y_true) / y_true.shape[0]\n",
        "\n",
        "def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculates Categorical Cross-Entropy loss.\"\"\"\n",
        "    # Clip predictions to avoid log(0)\n",
        "    epsillon = 1e-9\n",
        "    y_pred_clipped = np.clip(y_pred, epsillon, 1 - epsillon)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))\n",
        "\n",
        "def categorical_cross_entropy_gradient(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates the gradient of CCE loss w.r.t y_pred.\n",
        "    When combined with Softmax, the gradient w.r.t the Softmax *input* simplifies.\n",
        "    \"\"\"\n",
        "    return (y_pred - y_true) / y_true.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "LOSS_FUNCTIONS: Dict[str, Callable] = {\n",
        "    'mse': mean_squared_error,\n",
        "    'categorical_crossentropy': categorical_cross_entropy,\n",
        "}\n",
        "\n",
        "LOSS_GRADIENTS: Dict[str, Callable] = {\n",
        "    'mse': mean_squared_error_gradient,\n",
        "    'categorical_crossentropy': categorical_cross_entropy_gradient,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def _sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns a much stable Sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def _sigmoid_derivative(output: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Derivative of sigmoid (using its output).\"\"\"\n",
        "    return output * (1 - output)\n",
        "\n",
        "def _relu(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Rectified Linear Unit activation.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def _relu_derivative(output: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Derivative of ReLU.\"\"\"\n",
        "    return np.where(output > 0, 1, 0) # This here is because the derivative at 0 is undefined\n",
        "\n",
        "def _leaky_relu(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Leaky Rectified Linear Unit activation.\"\"\"\n",
        "    return np.where(x > 0, x, x * LEAKY_RELU_ALPHA)\n",
        "\n",
        "def _leaky_relu_derivative(output: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Derivative of Leaky ReLU.\"\"\"\n",
        "    return np.where(output > 0, 1, LEAKY_RELU_ALPHA)\n",
        "\n",
        "def _softmax(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Softmax activation function.\"\"\"\n",
        "    Soft_epsilon = 1e-9\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + Soft_epsilon) # Add epsilon to stabilise the division..\n",
        "\n",
        "def _softmax_derivative_cross_entropy(output: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the gradient of the cross-entropy loss with respect to the\n",
        "    inputs of the softmax function (often denoted dL/dz).\n",
        "    This combined gradient is simply (output - y_true).\n",
        "    Note: This function isn't the derivative of softmax itself, but\n",
        "          the combined gradient needed for backprop when using softmax + cross-entropy.\n",
        "    The `output_gradient` passed to the Activation layer's backward pass in this case\n",
        "    should be y_true. The loss gradient calculation should return y_pred - y_true.\n",
        "    So, the backward pass of Softmax Activation simplifies.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def _linear(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Linear activation (identity).\n",
        "    For every parameter receieved, perform a linear transformation on it,\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "def _linear_derivative(output: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Derivative of linear activation.\n",
        "    (d/dx)x = 1\n",
        "    \"\"\"\n",
        "    return np.ones_like(output)\n",
        "\n",
        "ACTIVATION_FUNCTIONS: Dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
        "    'sigmoid': _sigmoid,\n",
        "    'relu': _relu,\n",
        "    'leaky_relu': _leaky_relu,\n",
        "    'softmax': _softmax,\n",
        "    'linear': _linear,\n",
        "}\n",
        "\n",
        "ACTIVATION_DERIVATIVES: Dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
        "    'sigmoid': _sigmoid_derivative,\n",
        "    'relu': _relu_derivative,\n",
        "    'leaky_relu': _leaky_relu_derivative,\n",
        "    'softmax': lambda output: output * (1-output),\n",
        "    'linear': _linear_derivative,\n",
        "}\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"Base class for all network layers.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.input: Optional[np.ndarray] = None\n",
        "        self.output: Optional[np.ndarray] = None\n",
        "        self._has_weights = False # Flag to indicate if layer has trainable weights\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the forward pass.\n",
        "        Each Laye, Convolutions and Dense and Flattened/MLP have their own Forward Propagation, so they will use that instead of a core implementation from Layer class\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: float, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass.\n",
        "        kwargs might include Adam parameters like t, beta1, beta2, epsilon.\n",
        "        So far, these are the arguments being used, if any is included, then might not be used..\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def has_weights(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if the layer has trainable weights.\n",
        "        Returns True if the weights are trainable, else false.\n",
        "        \"\"\"\n",
        "        return self._has_weights\n",
        "\n",
        "\n",
        "class Activation(Layer):\n",
        "    \"\"\"Applies an activation function element-wise.\"\"\"\n",
        "    def __init__(self, activation_name: str):\n",
        "        \"\"\"\n",
        "        Initialise activation layer.\n",
        "\n",
        "        :param activation_name: Name of the activation function\n",
        "                                ('sigmoid', 'relu', 'leaky_relu', 'softmax', 'linear').\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if activation_name not in ACTIVATION_FUNCTIONS:\n",
        "            raise ValueError(f\"Unknown activation function: '{activation_name}'\")\n",
        "        self.activation_name = activation_name\n",
        "        self.activation_func = ACTIVATION_FUNCTIONS[activation_name]\n",
        "        self.activation_derivative = ACTIVATION_DERIVATIVES.get(activation_name)\n",
        "        if self.activation_derivative is None and self.activation_name != 'softmax':\n",
        "             raise ValueError(f\"Derivative for '{activation_name}' not found.\")\n",
        "\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Perform the forward pass applying the activation function.\"\"\"\n",
        "        self.input = input_data\n",
        "        self.output = self.activation_func(input_data)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: Optional[float] = None, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the activation function.\n",
        "        :param output_gradient: Gradient from the next layer.\n",
        "        :param learning_rate: Not used for activation layer.\n",
        "        :param kwargs: May include y_true for Softmax+CCE simplification.\n",
        "        :return: Gradient with respect to the input of this layer.\n",
        "        \"\"\"\n",
        "        if self.activation_name == 'softmax':\n",
        "            return output_gradient\n",
        "        elif self.activation_derivative:\n",
        "             '''\n",
        "              Apply chain rule: dL/dx = dL/dy * dy/dx\n",
        "              where y = activation_func(x)\n",
        "              dy/dx is the activation_derivative evaluated at the output y (or input x sometimes)\n",
        "              '''\n",
        "            return output_gradient * self.activation_derivative(self.output)\n",
        "        else:\n",
        "            raise RuntimeError(f\"Cannot perform backward pass for {self.activation_name} without derivative.\")\n",
        "\n",
        "\n",
        "\n",
        "class Conv2D(Layer):\n",
        "    \"\"\"2D Convolutional Layer.\"\"\"\n",
        "    def __init__(self, input_shape: Tuple[int, int, int], kernel_size: Tuple[int, int], depth: int, padding_mode: str = 'valid', stride: int = 0):\n",
        "        \"\"\"\n",
        "        Initialize convolutional layer.\n",
        "        The Stride and padding input have just been recently added, and therefore might malfunction.(Not tested yet.)\n",
        "\n",
        "        :param input_shape: Shape of the input volume (height, width, channels).\n",
        "        :param kernel_size: Size of the convolution kernel (height, width).\n",
        "        :param depth: Number of kernels/filters (output depth).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._has_weights = True\n",
        "        self.input_height, self.input_width, self.input_channels = input_shape\n",
        "        self.kernel_height, self.kernel_width = kernel_size\n",
        "        self.depth = depth # Number of output filters\n",
        "\n",
        "        if not (isinstance(kernel_size, tuple) and len(kernel_size) == 2):\n",
        "             raise ValueError(\"kernel_size must be a tuple of two integers (height, width).\")\n",
        "        if not (isinstance(input_shape, tuple) and len(input_shape) == 3):\n",
        "             raise ValueError(\"input_shape must be a tuple of three integers (height, width, channels).\")\n",
        "\n",
        "        # Xavier initialization\n",
        "        self.kernels_shape = (self.kernel_height, self.kernel_width, self.input_channels, self.depth)\n",
        "        limit = np.sqrt(6 / (np.prod(kernel_size) * self.input_channels + np.prod(kernel_size) * self.depth))\n",
        "        self.kernels = np.random.uniform(-limit, limit, self.kernels_shape)\n",
        "        self.biases = np.zeros(self.depth) # One bias per output filter\n",
        "\n",
        "        # Adam optimizerz\n",
        "        self.m_kernels = np.zeros_like(self.kernels)\n",
        "        self.v_kernels = np.zeros_like(self.kernels)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        self.output_height = self.input_height - self.kernel_height + 1\n",
        "        self.output_width = self.input_width - self.kernel_width + 1\n",
        "        if self.output_height <= 0 or self.output_width <= 0:\n",
        "            raise ValueError(f\"Kernel size {kernel_size} is too large for input shape {input_shape[:2]}.\")\n",
        "        self.output_shape = (self.output_height, self.output_width, self.depth)\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the forward pass using convolution.\n",
        "\n",
        "        Note: This implementation uses scipy.signal.convolve2d which is\n",
        "              computationally expensive for large inputs/kernels compared to\n",
        "              optimized libraries like im2col\n",
        "\n",
        "        :param input_data: Input data of shape (batch_size, height, width, channels).\n",
        "        :return: Output feature map of shape (batch_size, new_height, new_width, depth).\n",
        "        \"\"\"\n",
        "        self.input = input_data\n",
        "        batch_size = input_data.shape[0]\n",
        "\n",
        "        # Initialize output array\n",
        "        self.output = np.zeros((batch_size, *self.output_shape))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for d in range(self.depth):\n",
        "                output_feature_map = np.zeros((self.output_height, self.output_width))\n",
        "                for c in range(self.input_channels):\n",
        "                    # Kernel shape: (kH, kW, InChannels, OutDepth)\n",
        "                    kernel_slice = self.kernels[:, :, c, d]\n",
        "                    input_slice = self.input[i, :, :, c]\n",
        "                    output_feature_map += scipy.signal.convolve2d(\n",
        "                        input_slice, kernel_slice, mode=self.padding_mode\n",
        "                    )\n",
        "                # Add bias\n",
        "                self.output[i, :, :, d] = output_feature_map + self.biases[d]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def _adam_update(self, param: np.ndarray, grad: np.ndarray, m: np.ndarray, v: np.ndarray,\n",
        "                     learning_rate: float, t: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Helper function to perform Adam update.\"\"\"\n",
        "        m = ADAM_BETA1 * m + (1 - ADAM_BETA1) * grad\n",
        "        v = ADAM_BETA2 * v + (1 - ADAM_BETA2) * (grad ** 2)\n",
        "\n",
        "        m_hat = m / (1 - ADAM_BETA1 ** t)\n",
        "        v_hat = v / (1 - ADAM_BETA2 ** t)\n",
        "\n",
        "        # Update\n",
        "        param -= learning_rate * m_hat / (np.sqrt(v_hat) + ADAM_EPSILON)\n",
        "        return param, m, v\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: float, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass to compute gradients and update weights using Adam.\n",
        "\n",
        "        Note: This implementation uses scipy.signal correlate2d/convolve2d,\n",
        "              which can be slow.\n",
        "\n",
        "        :param output_gradient: Gradient from the next layer, shape (batch_size, out_h, out_w, depth).\n",
        "        :param learning_rate: Learning rate for the optimizer.\n",
        "        :param kwargs: Expected to contain 't' (Adam timestep).\n",
        "        :return: Gradient with respect to the input of this layer.\n",
        "        \"\"\"\n",
        "        if 't' not in kwargs:\n",
        "            raise ValueError(\"Adam timestep 't' is required for backward pass.\")\n",
        "        t = kwargs['t']\n",
        "\n",
        "        batch_size = output_gradient.shape[0]\n",
        "        kernels_gradient = np.zeros_like(self.kernels)\n",
        "        biases_gradient = np.zeros_like(self.biases)\n",
        "        input_gradient = np.zeros_like(self.input)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for d in range(self.depth):\n",
        "                biases_gradient[d] += np.sum(output_gradient[i, :, :, d])\n",
        "\n",
        "                for c in range(self.input_channels):\n",
        "                    input_slice = self.input[i, :, :, c]\n",
        "                    output_grad_slice = output_gradient[i, :, :, d]\n",
        "                    kernels_gradient[:, :, c, d] += scipy.signal.correlate2d(\n",
        "                        input_slice, output_grad_slice, mode='valid'\n",
        "                    )\n",
        "\n",
        "                    kernel_slice = self.kernels[:, :, c, d]\n",
        "                    rotated_kernel = np.rot90(kernel_slice, 2) # Rotate 180 degrees\n",
        "                    input_gradient[i, :, :, c] += scipy.signal.convolve2d(\n",
        "                        output_grad_slice, rotated_kernel, mode='full'\n",
        "                    )\n",
        "\n",
        "        # Update kernels and biases using Adam\n",
        "        self.kernels, self.m_kernels, self.v_kernels = self._adam_update(\n",
        "            self.kernels, kernels_gradient, self.m_kernels, self.v_kernels, learning_rate, t\n",
        "        )\n",
        "        self.biases, self.m_biases, self.v_biases = self._adam_update(\n",
        "            self.biases, biases_gradient, self.m_biases, self.v_biases, learning_rate, t\n",
        "        )\n",
        "\n",
        "        return input_gradient\n",
        "\n",
        "\n",
        "class MaxPool2D(Layer):\n",
        "    \"\"\"2D Max Pooling Layer.\"\"\"\n",
        "    def __init__(self, pool_size: Tuple[int, int] = (2, 2), stride: Optional[Tuple[int, int]] = None):\n",
        "        \"\"\"\n",
        "        Initialize max pooling layer.\n",
        "\n",
        "        :param pool_size: Size of the pooling window (height, width).\n",
        "        :param stride: Step size for pooling. If None, defaults to pool_size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.pool_height, self.pool_width = pool_size\n",
        "        self.stride_h, self.stride_w = stride if stride is not None else pool_size\n",
        "        self.max_indices: Optional[np.ndarray] = None\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the forward pass using max pooling.\n",
        "\n",
        "        :param input_data: Input data of shape (batch_size, height, width, channels).\n",
        "        :return: Output after max pooling.\n",
        "        \"\"\"\n",
        "        self.input = input_data\n",
        "        batch_size, h_in, w_in, channels = input_data.shape\n",
        "\n",
        "        h_out = (h_in - self.pool_height) // self.stride_h + 1\n",
        "        w_out = (w_in - self.pool_width) // self.stride_w + 1\n",
        "        if h_out <= 0 or w_out <= 0:\n",
        "            raise ValueError(f\"Pool size {self.pool_height, self.pool_width} with stride {self.stride_h, self.stride_w} \"\n",
        "                             f\"is too large for input shape {h_in, w_in}.\")\n",
        "\n",
        "        output = np.zeros((batch_size, h_out, w_out, channels))\n",
        "\n",
        "        self.max_indices = np.zeros((batch_size, h_out, w_out, channels, 2), dtype=int)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h_out):\n",
        "                    for j in range(w_out):\n",
        "                        h_start = i * self.stride_h\n",
        "                        h_end = h_start + self.pool_height\n",
        "                        w_start = j * self.stride_w\n",
        "                        w_end = w_start + self.pool_width\n",
        "\n",
        "                        pool_region = input_data[b, h_start:h_end, w_start:w_end, c]\n",
        "\n",
        "                        max_val = np.max(pool_region)\n",
        "                        max_pos_relative = np.unravel_index(np.argmax(pool_region), pool_region.shape)\n",
        "\n",
        "                        output[b, i, j, c] = max_val\n",
        "                        self.max_indices[b, i, j, c] = max_pos_relative\n",
        "\n",
        "        self.output = output\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: Optional[float] = None, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass for max pooling.\n",
        "\n",
        "        Distributes the gradient only to the locations where the max value was originally found.\n",
        "\n",
        "        :param output_gradient: Gradient from the next layer.\n",
        "        :param learning_rate: Not used for pooling layer.\n",
        "        :return: Gradient with respect to the input of this layer.\n",
        "        \"\"\"\n",
        "        if self.input is None or self.max_indices is None:\n",
        "            raise RuntimeError(\"Forward pass must be called before backward pass.\")\n",
        "\n",
        "        batch_size, h_out, w_out, channels = output_gradient.shape\n",
        "        input_gradient = np.zeros_like(self.input)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(h_out):\n",
        "                    for j in range(w_out):\n",
        "                      # Window Cordinates\n",
        "                        h_start = i * self.stride_h\n",
        "                        w_start = j * self.stride_w\n",
        "\n",
        "                        h_max_rel, w_max_rel = self.max_indices[b, i, j, c]\n",
        "\n",
        "                        h_abs = h_start + h_max_rel\n",
        "                        w_abs = w_start + w_max_rel\n",
        "\n",
        "                        input_gradient[b, h_abs, w_abs, c] += output_gradient[b, i, j, c]\n",
        "\n",
        "        return input_gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Flatten(Layer):\n",
        "    \"\"\"Flattens the input volume into a vector.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.original_shape: Optional[Tuple[int, ...]] = None\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the forward pass, flattening the input.\n",
        "\n",
        "        :param input_data: Input data of shape (batch_size, height, width, channels) or similar.\n",
        "        :return: Flattened data of shape (batch_size, height * width * channels).\n",
        "        \"\"\"\n",
        "        self.input = input_data\n",
        "        self.original_shape = input_data.shape\n",
        "        batch_size = input_data.shape[0]\n",
        "\n",
        "        flattened_dim = np.prod(input_data.shape[1:])\n",
        "\n",
        "        self.output = input_data.reshape(batch_size, flattened_dim)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: Optional[float] = None, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass, reshaping the gradient back to the original input shape.\n",
        "\n",
        "        :param output_gradient: Gradient from the next layer (flattened).\n",
        "        :param learning_rate: Not used for flatten layer.\n",
        "        :return: Gradient with respect to the input (reshaped).\n",
        "        \"\"\"\n",
        "        if self.original_shape is None:\n",
        "             raise RuntimeError(\"Forward pass must be called before backward pass.\")\n",
        "\n",
        "        return output_gradient.reshape(self.original_shape)\n",
        "\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    \"\"\"Dense (fully connected) layer.\"\"\"\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        \"\"\"\n",
        "        Initialize dense layer. Activation should be applied by a subsequent Activation layer.\n",
        "\n",
        "        :param input_size: Number of input features (neurons in the previous layer).\n",
        "        :param output_size: Number of output features (neurons in this layer).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._has_weights = True\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Xavier\n",
        "        limit = np.sqrt(6 / (input_size + output_size))\n",
        "        self.weights = np.random.uniform(-limit, limit, (input_size, output_size))\n",
        "        self.biases = np.zeros(output_size) # One bias per output neuron\n",
        "\n",
        "        # Adam\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "\n",
        "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the forward pass (linear transformation Wx + b).\n",
        "\n",
        "        :param input_data: Input data of shape (batch_size, input_size).\n",
        "        :return: Output of shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(input_data, self.weights) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    def _adam_update(self, param: np.ndarray, grad: np.ndarray, m: np.ndarray, v: np.ndarray,\n",
        "                     learning_rate: float, t: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Helper function to perform Adam update.\"\"\"\n",
        "        m = ADAM_BETA1 * m + (1 - ADAM_BETA1) * grad\n",
        "        v = ADAM_BETA2 * v + (1 - ADAM_BETA2) * (grad ** 2)\n",
        "\n",
        "        m_hat = m / (1 - ADAM_BETA1 ** t)\n",
        "        v_hat = v / (1 - ADAM_BETA2 ** t)\n",
        "\n",
        "        # Update parameter\n",
        "        param -= learning_rate * m_hat / (np.sqrt(v_hat) + ADAM_EPSILON)\n",
        "        return param, m, v\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: float, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform the backward pass for the dense layer using Adam optimiser.\n",
        "\n",
        "        Notes: This gradient `output_gradient` is dL/dz where z is the output of this Dense layer\n",
        "              (BEFORE any activation is applied). It comes from the layer before Activation layer's\n",
        "              backward pass.\n",
        "\n",
        "        :param output_gradient: Gradient from the next layer (typically an Activation layer).\n",
        "                                Shape: (batch_size, output_size).\n",
        "        :param learning_rate: Learning rate for the optimizer.\n",
        "        :param kwargs: Expected to contain 't' (Adam timestep).\n",
        "        :return: Gradient with respect to the input of this layer (dL/dx).\n",
        "                 Shape: (batch_size, input_size).\n",
        "        \"\"\"\n",
        "        if 't' not in kwargs:\n",
        "            raise ValueError(\"Adam timestep 't' is required for back pass.\")\n",
        "        if self.input is None:\n",
        "             raise RuntimeError(\"Forward pass must be called before backward pass.\")\n",
        "        t = kwargs['t']\n",
        "\n",
        "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
        "\n",
        "        biases_gradient = np.sum(output_gradient, axis=0)\n",
        "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
        "        self.weights, self.m_weights, self.v_weights = self._adam_update(\n",
        "            self.weights, weights_gradient, self.m_weights, self.v_weights, learning_rate, t\n",
        "        )\n",
        "        self.biases, self.m_biases, self.v_biases = self._adam_update(\n",
        "            self.biases, biases_gradient, self.m_biases, self.v_biases, learning_rate, t\n",
        "        )\n",
        "\n",
        "        return input_gradient\n",
        "\n",
        "class LSTM(Layer):\n",
        "    \"\"\"LSTM Layer (supports sequence inputs).\"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        \"\"\"\n",
        "        Initialises LSTM.\n",
        "        :param input_size: Number of input features.\n",
        "        :param hidden_size: Number of hidden units.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._has_weights = True\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Xavier init for weights\n",
        "        # Maybe later on use a different weight initialiser\n",
        "        limit = np.sqrt(1.0 / (input_size + hidden_size))\n",
        "        self.W_f = np.random.uniform(-limit, limit, (input_size + hidden_size, hidden_size))\n",
        "        self.W_i = np.random.uniform(-limit, limit, (input_size + hidden_size, hidden_size))\n",
        "        self.W_c = np.random.uniform(-limit, limit, (input_size + hidden_size, hidden_size))\n",
        "        self.W_o = np.random.uniform(-limit, limit, (input_size + hidden_size, hidden_size))\n",
        "\n",
        "        self.b_f = np.zeros(hidden_size)\n",
        "        self.b_i = np.zeros(hidden_size)\n",
        "        self.b_c = np.zeros(hidden_size)\n",
        "        self.b_o = np.zeros(hidden_size)\n",
        "\n",
        "        # Adam optimiser statess\n",
        "        self.m, self.v = {}, {}\n",
        "        for name in [\"W_f\",\"W_i\",\"W_c\",\"W_o\",\"b_f\",\"b_i\",\"b_c\",\"b_o\"]:\n",
        "            self.m[name] = np.zeros_like(getattr(self, name))\n",
        "            self.v[name] = np.zeros_like(getattr(self, name))\n",
        "\n",
        "    def forward(self, input_seq: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass for the whole sequence.\n",
        "\n",
        "        :param input_seq: Input shape (batch_size, seq_len, input_size)\n",
        "        :return: Hidden states for each step (batch_size, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = input_seq.shape\n",
        "        self.input = input_seq\n",
        "\n",
        "        # Initialize hidden and cell states\n",
        "        self.h = np.zeros((batch_size, seq_len, self.hidden_size))\n",
        "        self.c = np.zeros((batch_size, seq_len, self.hidden_size))\n",
        "\n",
        "        self.cache = []  # store gates for backward\n",
        "\n",
        "        h_t = np.zeros((batch_size, self.hidden_size))\n",
        "        c_t = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = input_seq[:, t, :]\n",
        "            concat = np.concatenate([h_t, x_t], axis=1)\n",
        "\n",
        "            f_t = _sigmoid(np.dot(concat, self.W_f) + self.b_f)\n",
        "            i_t = _sigmoid(np.dot(concat, self.W_i) + self.b_i)\n",
        "            c_hat_t = np.tanh(np.dot(concat, self.W_c) + self.b_c)\n",
        "            c_t = f_t * c_t + i_t * c_hat_t\n",
        "            o_t = _sigmoid(np.dot(concat, self.W_o) + self.b_o)\n",
        "            h_t = o_t * np.tanh(c_t)\n",
        "\n",
        "            self.h[:, t, :] = h_t\n",
        "            self.c[:, t, :] = c_t\n",
        "            self.cache.append((concat, f_t, i_t, c_hat_t, c_t, o_t, h_t))\n",
        "\n",
        "        self.output = self.h\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient: np.ndarray, learning_rate: float, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass through LSTM (simplified, no peepholes).\n",
        "        :param output_gradient: Gradient w.r.t. hidden states (batch, seq_len, hidden_size)\n",
        "        :return: Gradient w.r.t. input sequence (batch, seq_len, input_size)\n",
        "        \"\"\"\n",
        "        if 't' not in kwargs:\n",
        "            raise ValueError(\"Adam timestep 't' is required for backward pass.\")\n",
        "        t_step = kwargs['t']\n",
        "\n",
        "        batch_size, seq_len, _ = output_gradient.shape\n",
        "        dx = np.zeros((batch_size, seq_len, self.input_size))\n",
        "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
        "        dc_next = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        # Gradients for parameters\n",
        "        grads = {name: np.zeros_like(getattr(self, name)) for name in self.m}\n",
        "\n",
        "        for t in reversed(range(seq_len)):\n",
        "            concat, f_t, i_t, c_hat_t, c_t, o_t, h_t = self.cache[t]\n",
        "\n",
        "            dh = output_gradient[:, t, :] + dh_next\n",
        "            do = dh * np.tanh(c_t) * o_t * (1 - o_t)\n",
        "            dc = dh * o_t * (1 - np.tanh(c_t)**2) + dc_next\n",
        "            di = dc * c_hat_t * i_t * (1 - i_t)\n",
        "            dc_hat = dc * i_t * (1 - c_hat_t**2)\n",
        "            df = dc * self.c[:, t-1, :] * f_t * (1 - f_t) if t > 0 else 0\n",
        "\n",
        "            dconcat = (np.dot(do, self.W_o.T) +\n",
        "                       np.dot(di, self.W_i.T) +\n",
        "                       np.dot(dc_hat, self.W_c.T) +\n",
        "                       (np.dot(df, self.W_f.T) if isinstance(df, np.ndarray) else 0))\n",
        "\n",
        "            dh_next = dconcat[:, :self.hidden_size]\n",
        "            dc_next = dc * f_t\n",
        "            dx[:, t, :] = dconcat[:, self.hidden_size:]\n",
        "\n",
        "            grads[\"W_o\"] += np.dot(concat.T, do)\n",
        "            grads[\"W_i\"] += np.dot(concat.T, di)\n",
        "            grads[\"W_c\"] += np.dot(concat.T, dc_hat)\n",
        "            if isinstance(df, np.ndarray):\n",
        "                grads[\"W_f\"] += np.dot(concat.T, df)\n",
        "\n",
        "            grads[\"b_o\"] += np.sum(do, axis=0)\n",
        "            grads[\"b_i\"] += np.sum(di, axis=0)\n",
        "            grads[\"b_c\"] += np.sum(dc_hat, axis=0)\n",
        "            if isinstance(df, np.ndarray):\n",
        "                grads[\"b_f\"] += np.sum(df, axis=0)\n",
        "\n",
        "        # Adam update\n",
        "        for name in grads:\n",
        "            self.__dict__[name], self.m[name], self.v[name] = self._adam_update(\n",
        "                self.__dict__[name], grads[name], self.m[name], self.v[name], learning_rate, t_step\n",
        "            )\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def _adam_update(self, param, grad, m, v, lr, t):\n",
        "        m = ADAM_BETA1 * m + (1 - ADAM_BETA1) * grad\n",
        "        v = ADAM_BETA2 * v + (1 - ADAM_BETA2) * (grad ** 2)\n",
        "        m_hat = m / (1 - ADAM_BETA1 ** t)\n",
        "        v_hat = v / (1 - ADAM_BETA2 ** t)\n",
        "        param -= lr * m_hat / (np.sqrt(v_hat) + ADAM_EPSILON)\n",
        "        return param, m, v\n",
        "\n",
        "\n"
      ]
    }
  ]
}