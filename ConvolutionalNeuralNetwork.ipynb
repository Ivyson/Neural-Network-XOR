{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b024e3b8",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) derive their name from the convolution operation that occurs within them. This type of neural network builds upon the Multi-Layer Perceptron (MLP) model. \n",
    "\n",
    "## Why CNNs?\n",
    "The limitation of MLPs is that they only work with flattened arrays of data. For example, an image must be flattened into a one-dimensional array to be processed by an MLP. However, this flattening process often results in the loss of spatial information. Additionally, most real-world data, such as images and audio, is multi-dimensional. \n",
    "\n",
    "### Example:\n",
    "- Images are represented as matrices (e.g., a 1020x720 image has 1020 rows and 720 columns of pixels).\n",
    "- Each pixel has an RGB value, adding a third dimension to the data.\n",
    "\n",
    "Flattening such data into a single array can be computationally expensive and inefficient. CNNs address these challenges by preserving the spatial structure of the data and efficiently processing multi-dimensional inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## CNN Architecture\n",
    "\n",
    "CNNs are specialized neural networks designed for processing data with a grid-like topology, such as images. They consist of the following layers:\n",
    "\n",
    "### 1. **Convolutional Layers**\n",
    "- These layers apply convolutional filters (kernels) to the input data to extract local features such as edges, textures, and patterns.\n",
    "- The kernels contain weights that are learned through backpropagation, similar to the MLP model.\n",
    "\n",
    "### 2. **Pooling Layers**\n",
    "- Pooling layers perform sub-sampling or down-sampling, reducing the dimensions of the input data. This helps the network recognize objects even when they are deformed or appear in different lighting conditions.\n",
    "- **Max Pooling** is a common pooling technique. It extracts the maximum value within a selected region of the feature map.\n",
    "\n",
    "#### Example of Max Pooling:\n",
    "**Input Feature Map**:\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 2 & 4 & 3 \\\\\n",
    "6 & 7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "If we perform max pooling with a stride of 2, the output feature map will be:\n",
    "\n",
    "**Output Feature Map**:\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "9 & 9\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### 3. **Fully Connected Layers**\n",
    "- These layers are similar to those in MLPs and are typically used for the final output.\n",
    "- They are used for tasks such as:\n",
    "    - **Classification**: Predicting categories.\n",
    "    - **Regression**: Predicting continuous values.\n",
    "    - **Probability Estimation**: Outputting probabilities for different classes.\n",
    "\n",
    "### 4. **Activation Layers**\n",
    "- Activation layers, such as ReLU (Rectified Linear Unit), introduce non-linearity into the model.\n",
    "- They can down-sample the output from previous layers into a range (e.g., 0 to 1) or compute binary values, depending on the task.\n",
    "\n",
    "---\n",
    "\n",
    "CNNs are powerful tools for processing multi-dimensional data, especially images, and have become a cornerstone of modern deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5485006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.2587\n",
      "Epoch 11/50, Loss: 0.2111\n",
      "Epoch 11/50, Loss: 0.2111\n",
      "Epoch 21/50, Loss: 0.1373\n",
      "Epoch 21/50, Loss: 0.1373\n",
      "Epoch 31/50, Loss: 0.0936\n",
      "Epoch 31/50, Loss: 0.0936\n",
      "Epoch 41/50, Loss: 0.0889\n",
      "Epoch 41/50, Loss: 0.0889\n",
      "Epoch 50/50, Loss: 0.0505\n",
      "Predictions:\n",
      "[[0.95822283]\n",
      " [0.15689718]\n",
      " [0.88956841]\n",
      " [0.05826617]\n",
      " [0.85981537]]\n",
      "Actual:\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Epoch 50/50, Loss: 0.0505\n",
      "Predictions:\n",
      "[[0.95822283]\n",
      " [0.15689718]\n",
      " [0.88956841]\n",
      " [0.05826617]\n",
      " [0.85981537]]\n",
      "Actual:\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import pickle\n",
    "import os\n",
    "from typing import Union, List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Base Layer Class for all layer types\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Forward pass - to be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # Backward pass - to be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Convolutional Layer\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        \"\"\"\n",
    "        Initialize convolutional layer\n",
    "        \n",
    "        :param input_shape: (height, width, channels)\n",
    "        :param kernel_size: Size of the convolution kernel (height, width)\n",
    "        :param depth: Number of kernels/filters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.input_height, self.input_width, self.input_channels = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Initialize filters with Xavier/Glorot initialization\n",
    "        self.kernels_shape = (kernel_size[0], kernel_size[1], self.input_channels, depth)\n",
    "        limit = np.sqrt(6 / (np.prod(kernel_size) * self.input_channels + np.prod(kernel_size) * depth))\n",
    "        self.kernels = np.random.uniform(-limit, limit, self.kernels_shape)\n",
    "        self.biases = np.zeros(depth)\n",
    "        \n",
    "        # For Adam optimizer\n",
    "        self.m_kernels = np.zeros_like(self.kernels)\n",
    "        self.v_kernels = np.zeros_like(self.kernels)\n",
    "        self.m_biases = np.zeros_like(self.biases)\n",
    "        self.v_biases = np.zeros_like(self.biases)\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        self.output_shape = (\n",
    "            self.input_height - kernel_size[0] + 1,\n",
    "            self.input_width - kernel_size[1] + 1,\n",
    "            depth\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for convolutional layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Output of shape (batch_size, new_height, new_width, depth)\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        batch_size = input_data.shape[0]\n",
    "        \n",
    "        # Initialize output array\n",
    "        self.output = np.zeros((batch_size, *self.output_shape))\n",
    "        \n",
    "        # Perform convolution for each sample in batch\n",
    "        for i in range(batch_size):\n",
    "            for d in range(self.depth):\n",
    "                for c in range(self.input_channels):\n",
    "                    # Convolve each channel with corresponding kernel\n",
    "                    self.output[i, :, :, d] += scipy.signal.convolve2d(\n",
    "                        self.input[i, :, :, c], \n",
    "                        self.kernels[:, :, c, d], \n",
    "                        mode='valid'\n",
    "                    )\n",
    "                # Add bias\n",
    "                self.output[i, :, :, d] += self.biases[d]\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \"\"\"\n",
    "        Backward pass for convolutional layer using Adam optimizer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer of shape (batch_size, height, width, depth)\n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        :param beta1: Exponential decay rate for 1st moment estimates\n",
    "        :param beta2: Exponential decay rate for 2nd moment estimates\n",
    "        :param epsilon: Small constant for numerical stability\n",
    "        :param t: Timestep (for bias correction in Adam)\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size = output_gradient.shape[0]\n",
    "        kernels_gradient = np.zeros_like(self.kernels)\n",
    "        biases_gradient = np.zeros_like(self.biases)\n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "        \n",
    "        # Calculate gradients for each sample in batch\n",
    "        for i in range(batch_size):\n",
    "            for d in range(self.depth):\n",
    "                # Gradient for biases - simple sum over height and width dimensions\n",
    "                biases_gradient[d] += np.sum(output_gradient[i, :, :, d])\n",
    "                \n",
    "                for c in range(self.input_channels):\n",
    "                    # Gradient for kernels - correlation between input and output gradient\n",
    "                    kernels_gradient[:, :, c, d] += scipy.signal.correlate2d(\n",
    "                        self.input[i, :, :, c],\n",
    "                        output_gradient[i, :, :, d],\n",
    "                        mode='valid'\n",
    "                    )\n",
    "                    \n",
    "                    # Gradient for input - full convolution with rotated kernel\n",
    "                    rotated_kernel = np.rot90(self.kernels[:, :, c, d], 2)\n",
    "                    input_gradient[i, :, :, c] += scipy.signal.convolve2d(\n",
    "                        output_gradient[i, :, :, d],\n",
    "                        rotated_kernel,\n",
    "                        mode='full'\n",
    "                    )\n",
    "        \n",
    "        # Update kernels and biases using Adam optimizer\n",
    "        # For kernels\n",
    "        self.m_kernels = beta1 * self.m_kernels + (1 - beta1) * kernels_gradient\n",
    "        self.v_kernels = beta2 * self.v_kernels + (1 - beta2) * (kernels_gradient ** 2)\n",
    "        m_hat_kernels = self.m_kernels / (1 - beta1 ** t)\n",
    "        v_hat_kernels = self.v_kernels / (1 - beta2 ** t)\n",
    "        self.kernels -= learning_rate * m_hat_kernels / (np.sqrt(v_hat_kernels) + epsilon)\n",
    "        \n",
    "        # For biases\n",
    "        self.m_biases = beta1 * self.m_biases + (1 - beta1) * biases_gradient\n",
    "        self.v_biases = beta2 * self.v_biases + (1 - beta2) * (biases_gradient ** 2)\n",
    "        m_hat_biases = self.m_biases / (1 - beta1 ** t)\n",
    "        v_hat_biases = self.v_biases / (1 - beta2 ** t)\n",
    "        self.biases -= learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# MaxPooling Layer\n",
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), stride=None):\n",
    "        \"\"\"\n",
    "        Initialize max pooling layer\n",
    "        \n",
    "        :param pool_size: Size of the pooling window (height, width)\n",
    "        :param stride: Stride of the pooling operation, defaults to pool_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "        self.max_indices = None  # To store indices of max values for backprop\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for max pooling layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Output after max pooling\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        batch_size, h_in, w_in, channels = input_data.shape\n",
    "        h_pool, w_pool = self.pool_size\n",
    "        h_stride, w_stride = self.stride\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        h_out = (h_in - h_pool) // h_stride + 1\n",
    "        w_out = (w_in - w_pool) // w_stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, h_out, w_out, channels))\n",
    "        self.max_indices = np.zeros((batch_size, h_out, w_out, channels, 2), dtype=int)\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for c in range(channels):\n",
    "                        h_start = i * h_stride\n",
    "                        h_end = h_start + h_pool\n",
    "                        w_start = j * w_stride\n",
    "                        w_end = w_start + w_pool\n",
    "                        \n",
    "                        # Get the region to pool from\n",
    "                        pool_region = input_data[b, h_start:h_end, w_start:w_end, c]\n",
    "                        \n",
    "                        # Find max value and its position within the pool region\n",
    "                        max_val = np.max(pool_region)\n",
    "                        max_pos = np.unravel_index(np.argmax(pool_region), pool_region.shape)\n",
    "                        \n",
    "                        # Store max value and its position for backprop\n",
    "                        output[b, i, j, c] = max_val\n",
    "                        self.max_indices[b, i, j, c] = max_pos\n",
    "        \n",
    "        self.output = output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for max pooling layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for pooling layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size, h_out, w_out, channels = output_gradient.shape\n",
    "        h_in, w_in = self.input.shape[1:3]\n",
    "        h_pool, w_pool = self.pool_size\n",
    "        h_stride, w_stride = self.stride\n",
    "        \n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "        \n",
    "        # Distribute gradient only to max elements\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for c in range(channels):\n",
    "                        h_start = i * h_stride\n",
    "                        w_start = j * w_stride\n",
    "                        h_max, w_max = self.max_indices[b, i, j, c]\n",
    "                        \n",
    "                        # Add gradient to the position where the max was found\n",
    "                        input_gradient[b, h_start + h_max, w_start + w_max, c] += output_gradient[b, i, j, c]\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# Flatten Layer\n",
    "class Flatten(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for flatten layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Flattened data of shape (batch_size, height*width*channels)\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.input_shape = input_data.shape\n",
    "        batch_size = input_data.shape[0]\n",
    "        flattened_dim = np.prod(input_data.shape[1:])\n",
    "        \n",
    "        self.output = input_data.reshape(batch_size, flattened_dim)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for flatten layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for flatten layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        return output_gradient.reshape(self.input_shape)\n",
    "\n",
    "# Dense (Fully Connected) Layer\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize dense (fully connected) layer\n",
    "        \n",
    "        :param input_size: Number of input features\n",
    "        :param output_size: Number of output features\n",
    "        :param activation: Activation function ('sigmoid', 'relu', 'leaky_relu', 'linear')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "        # For Adam optimizer\n",
    "        self.m_weights = np.zeros_like(self.weights)\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.m_biases = np.zeros_like(self.biases)\n",
    "        self.v_biases = np.zeros_like(self.biases)\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activation_func = self._get_activation(self.activation_type)\n",
    "        self.activation_derivative = self._get_activation_derivative(self.activation_type)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        elif name == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif name == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, x, x * 0.01)\n",
    "        elif name == 'softmax':\n",
    "            def softmax(x):\n",
    "                exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "                return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
    "            return softmax\n",
    "        elif name == 'linear':\n",
    "            return lambda x: x\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: '{name}'\")\n",
    "    \n",
    "    def _get_activation_derivative(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda x: x * (1 - x)\n",
    "        elif name == 'relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif name == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif name == 'linear':\n",
    "            return lambda x: np.ones_like(x)\n",
    "        elif name == 'softmax':\n",
    "            return lambda x: x * (1 - x)  # Simplified for when used with cross-entropy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function derivative: '{name}'\")\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for dense layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, input_size)\n",
    "        :return: Output after dense layer and activation\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.z = np.dot(input_data, self.weights) + self.biases\n",
    "        self.output = self.activation_func(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \"\"\"\n",
    "        Backward pass for dense layer using Adam optimizer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        :param beta1: Exponential decay rate for 1st moment estimates\n",
    "        :param beta2: Exponential decay rate for 2nd moment estimates\n",
    "        :param epsilon: Small constant for numerical stability\n",
    "        :param t: Timestep (for bias correction in Adam)\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        # Calculate gradient through activation function\n",
    "        if self.activation_type == 'softmax':\n",
    "            # Special case for softmax (assuming cross-entropy loss)\n",
    "            delta = output_gradient\n",
    "        else:\n",
    "            delta = output_gradient * self.activation_derivative(self.output)\n",
    "        \n",
    "        # Calculate gradients for weights and biases\n",
    "        weights_gradient = np.dot(self.input.T, delta)\n",
    "        biases_gradient = np.sum(delta, axis=0)\n",
    "        \n",
    "        # Calculate gradient to pass to previous layer\n",
    "        input_gradient = np.dot(delta, self.weights.T)\n",
    "        \n",
    "        # Update weights and biases using Adam optimizer\n",
    "        # For weights\n",
    "        self.m_weights = beta1 * self.m_weights + (1 - beta1) * weights_gradient\n",
    "        self.v_weights = beta2 * self.v_weights + (1 - beta2) * (weights_gradient ** 2)\n",
    "        m_hat_weights = self.m_weights / (1 - beta1 ** t)\n",
    "        v_hat_weights = self.v_weights / (1 - beta2 ** t)\n",
    "        self.weights -= learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + epsilon)\n",
    "        \n",
    "        # For biases\n",
    "        self.m_biases = beta1 * self.m_biases + (1 - beta1) * biases_gradient\n",
    "        self.v_biases = beta2 * self.v_biases + (1 - beta2) * (biases_gradient ** 2)\n",
    "        m_hat_biases = self.m_biases / (1 - beta1 ** t)\n",
    "        v_hat_biases = self.v_biases / (1 - beta2 ** t)\n",
    "        self.biases -= learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# Activation Layer as a separate layer\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation):\n",
    "        \"\"\"\n",
    "        Initialize activation layer\n",
    "        \n",
    "        :param activation: Activation function name\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation_type = activation\n",
    "        self.activation_func = self._get_activation(activation)\n",
    "        self.activation_derivative = self._get_activation_derivative(activation)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        elif name == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif name == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, x, x * 0.01)\n",
    "        elif name == 'softmax':\n",
    "            def softmax(x):\n",
    "                exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "                return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
    "            return softmax\n",
    "        elif name == 'linear':\n",
    "            return lambda x: x\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: '{name}'\")\n",
    "    \n",
    "    def _get_activation_derivative(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda x: x * (1 - x)\n",
    "        elif name == 'relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif name == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif name == 'linear':\n",
    "            return lambda x: np.ones_like(x)\n",
    "        elif name == 'softmax':\n",
    "            return lambda x: x * (1 - x)  # Simplified for when used with cross-entropy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function derivative: '{name}'\")\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for activation layer\n",
    "        \n",
    "        :param input_data: Input data\n",
    "        :return: Output after activation\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = self.activation_func(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for activation layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for activation layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        if self.activation_type == 'softmax':\n",
    "            # Special case for softmax (assuming cross-entropy loss)\n",
    "            return output_gradient\n",
    "        return output_gradient * self.activation_derivative(self.output)\n",
    "\n",
    "# CNN Model\n",
    "class CNN:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize CNN model\n",
    "        \n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0  # Time step for Adam optimizer\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Add a layer to the model\n",
    "        \n",
    "        :param layer: Layer to add\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Make predictions with the model\n",
    "        \n",
    "        :param input_data: Input data\n",
    "        :return: Model predictions\n",
    "        \"\"\"\n",
    "        # Ensure input data is in batch format\n",
    "        if input_data.ndim == 3:  # Single image (height, width, channels)\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        :param X_train: Training data\n",
    "        :param y_train: Training labels\n",
    "        :param epochs: Number of epochs\n",
    "        :param batch_size: Batch size\n",
    "        :param verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        # Ensure X_train is in batch format\n",
    "        if X_train.ndim == 3:  # Single image (height, width, channels)\n",
    "            X_train = np.expand_dims(X_train, axis=0)\n",
    "        \n",
    "        # Ensure y_train is in batch format\n",
    "        if y_train.ndim == 1:  # Single label\n",
    "            y_train = np.expand_dims(y_train, axis=0)\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "            # Train in batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                # Get batch\n",
    "                X_batch = X_shuffled[i:min(i + batch_size, n_samples)]\n",
    "                y_batch = y_shuffled[i:min(i + batch_size, n_samples)]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.predict(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss += self._compute_loss(y_batch, output)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.t += 1  # Increment time step for Adam optimizer\n",
    "                grad = self._compute_loss_gradient(y_batch, output)\n",
    "                \n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad, self.learning_rate, t=self.t)\n",
    "            \n",
    "            # Average loss over all batches\n",
    "            loss /= n_samples / batch_size\n",
    "            \n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \n",
    "        :param y_true: True labels\n",
    "        :param y_pred: Predicted labels\n",
    "        :return: Loss value\n",
    "        \"\"\"\n",
    "        # Mean Squared Error\n",
    "        return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n",
    "    \n",
    "    def _compute_loss_gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss\n",
    "        \n",
    "        :param y_true: True labels\n",
    "        :param y_pred: Predicted labels\n",
    "        :return: Gradient of loss\n",
    "        \"\"\"\n",
    "        # Gradient of Mean Squared Error\n",
    "        return 2 * (y_pred - y_true) / y_true.shape[0]\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save model to file\n",
    "        \n",
    "        :param filename: Filename to save to\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filename):\n",
    "        \"\"\"\n",
    "        Load model from file\n",
    "        \n",
    "        :param filename: Filename to load from\n",
    "        :return: Loaded model\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "# Example usage with MNIST-like data (28x28)\n",
    "def generate_dummy_data(n_samples=100, img_size=(28, 28, 1)):\n",
    "    # Generate dummy images (random noise)\n",
    "    X = np.random.random((n_samples, *img_size))\n",
    "    # Generate dummy labels (binary classification)\n",
    "    y = np.random.randint(0, 2, (n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# Create a simple CNN model for binary classification\n",
    "def create_simple_cnn():\n",
    "    model = CNN(learning_rate=0.001)\n",
    "    \n",
    "    # Add layers\n",
    "    # Conv layer: input_shape=(28,28,1), kernel_size=(3,3), 8 filters\n",
    "    model.add(Conv2D(input_shape=(28, 28, 1), kernel_size=(3, 3), depth=8))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # MaxPooling layer: pool_size=(2,2)\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Conv layer: kernel_size=(3,3), 16 filters\n",
    "    model.add(Conv2D(input_shape=(13, 13, 8), kernel_size=(3, 3), depth=16))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # MaxPooling layer: pool_size=(2,2)\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer: 128 neurons\n",
    "    model.add(Dense(16 * 5 * 5, 128, activation='relu'))\n",
    "    \n",
    "    # Output layer: 1 neuron (binary classification)\n",
    "    model.add(Dense(128, 1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test with dummy data\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate dummy data\n",
    "    X, y = generate_dummy_data(n_samples=100)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_simple_cnn()\n",
    "    \n",
    "    # Train model\n",
    "    model.train(X, y, epochs=50, batch_size=10)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X[:5])\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)\n",
    "    print(\"Actual:\")\n",
    "    print(y[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
