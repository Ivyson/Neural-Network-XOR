{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b024e3b8",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) derive their name from the convolution operation that occurs within them. This type of neural network builds upon the Multi-Layer Perceptron (MLP) model. \n",
    "\n",
    "## Why CNNs?\n",
    "The limitation of MLPs is that they only work with flattened arrays of data. For example, an image must be flattened into a one-dimensional array to be processed by an MLP. However, this flattening process often results in the loss of spatial information. Additionally, most real-world data, such as images and audio, is multi-dimensional. \n",
    "\n",
    "### Example:\n",
    "- Images are represented as matrices (e.g., a 1020x720 image has 1020 rows and 720 columns of pixels).\n",
    "- Each pixel has an RGB value, adding a third dimension to the data.\n",
    "\n",
    "Flattening such data into a single array can be computationally expensive and inefficient. CNNs address these challenges by preserving the spatial structure of the data and efficiently processing multi-dimensional inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## CNN Architecture\n",
    "\n",
    "CNNs are specialized neural networks designed for processing data with a grid-like topology, such as images. They consist of the following layers:\n",
    "\n",
    "### 1. **Convolutional Layers**\n",
    "- These layers apply convolutional filters (kernels) to the input data to extract local features such as edges, textures, and patterns.\n",
    "- The kernels contain weights that are learned through backpropagation, similar to the MLP model.\n",
    "\n",
    "### 2. **Pooling Layers**\n",
    "- Pooling layers perform sub-sampling or down-sampling, reducing the dimensions of the input data. This helps the network recognize objects even when they are deformed or appear in different lighting conditions.\n",
    "- **Max Pooling** is a common pooling technique. It extracts the maximum value within a selected region of the feature map.\n",
    "\n",
    "#### Example of Max Pooling:\n",
    "**Input Feature Map**:\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 2 & 4 & 3 \\\\\n",
    "6 & 7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "If we perform max pooling with a stride of 2, the output feature map will be:\n",
    "\n",
    "**Output Feature Map**:\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "9 & 9\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### 3. **Fully Connected Layers**\n",
    "- These layers are similar to those in MLPs and are typically used for the final output.\n",
    "- They are used for tasks such as:\n",
    "    - **Classification**: Predicting categories.\n",
    "    - **Regression**: Predicting continuous values.\n",
    "    - **Probability Estimation**: Outputting probabilities for different classes.\n",
    "\n",
    "### 4. **Activation Layers**\n",
    "- Activation layers, such as ReLU (Rectified Linear Unit), introduce non-linearity into the model.\n",
    "- They can down-sample the output from previous layers into a range (e.g., 0 to 1) or compute binary values, depending on the task.\n",
    "\n",
    "---\n",
    "\n",
    "CNNs are powerful tools for processing multi-dimensional data, especially images, and have become a cornerstone of modern deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc65ff",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5485006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import pickle\n",
    "import os\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_nodes: np.ndarray,\n",
    "                 output_size: np.ndarray,\n",
    "                 learning_rate: Union[int, float] = 0.001, # This can be a float or an int, even though an int is not something i recommend.(I am a good engineer)\n",
    "                 activation:str = 'relu',\n",
    "                 output_activation:str ='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initializes the Neural Network with error checking for parameters.\n",
    "\n",
    "        :param input_size: Number of input features (must be positive integer)\n",
    "        :param hidden_nodes: Integer or List specifying number of neurons in each hidden layer (each must be positive integer)\n",
    "        :param output_size: Number of output neurons (must be positive integer)\n",
    "        :param learning_rate: Learning rate for optimizer (must be positive float)\n",
    "        :param activation: Activation function for hidden layers ('sigmoid', 'relu', 'leaky_relu', 'linear')\n",
    "        :param output_activation: Activation function for the output layer ('sigmoid', 'softmax', 'linear')\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If input types are incorrect.\n",
    "            ValueError: If input values are invalid (e.g., non-positive sizes, invalid activation names).\n",
    "        \"\"\"\n",
    "        # Input Validation\n",
    "        if not isinstance(input_size, int) or input_size <= 0:\n",
    "            raise ValueError(f\"input_size must be a positive integer, got {input_size}\")\n",
    "        if not isinstance(output_size, int) or output_size <= 0:\n",
    "            raise ValueError(f\"output_size must be a positive integer, got {output_size}\")\n",
    "        if not isinstance(learning_rate, (float, int)) or learning_rate <= 0:\n",
    "            raise ValueError(f\"learning_rate must be a positive number, got {learning_rate}\")\n",
    "        if not isinstance(activation, str):\n",
    "             raise TypeError(f\"activation must be a string, got {type(activation)}\")\n",
    "        if not isinstance(output_activation, str):\n",
    "             raise TypeError(f\"output_activation must be a string, got {type(output_activation)}\")\n",
    "\n",
    "        # alidate Hidden_nodes content\n",
    "        if isinstance(hidden_nodes, int):\n",
    "            if hidden_nodes <= 0:\n",
    "                 raise ValueError(f\"If hidden_nodes is an integer, it must be positive, got {hidden_nodes}\")\n",
    "            processed_hidden_nodes = [hidden_nodes] # convert single int to list\n",
    "        elif isinstance(hidden_nodes, list):\n",
    "            if not all(isinstance(n, int) and n > 0 for n in hidden_nodes):\n",
    "                 raise ValueError(f\"If hidden_nodes is a list, all elements must be positive integers, got {hidden_nodes}\")\n",
    "            processed_hidden_nodes = hidden_nodes # a list already..\n",
    "        else:\n",
    "            raise TypeError(f\"hidden_nodes must be a positive integer or a list of positive integers, got {type(hidden_nodes)}\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_nodes = processed_hidden_nodes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.activation_type = activation\n",
    "        self.output_activation_type = output_activation\n",
    "\n",
    "        # This part is safe now, the checks above have saved it...\n",
    "        layer_sizes = [self.input_size] + self.hidden_nodes + [self.output_size]\n",
    "        self.num_layers = len(layer_sizes)\n",
    "\n",
    "        # Init weights and\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Layer sizes are guaranteed positive ints here\n",
    "            # Now Check for potential division by zero\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i+1]\n",
    "            limit = np.sqrt(6 / (fan_in + fan_out)) # Use Xavier Method.. Safer..\n",
    "\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (fan_in, fan_out)))\n",
    "            self.biases.append(np.zeros(fan_out))\n",
    "\n",
    "        try:\n",
    "            self.activation_func = self._get_activation(self.activation_type)\n",
    "            self.activation_derivative = self._get_activation_derivative(self.activation_type)\n",
    "            self.output_activation_func = self._get_activation(self.output_activation_type)\n",
    "            self.output_activation_derivative = self._get_activation_derivative(self.output_activation_type)\n",
    "        except ValueError as e:\n",
    "             raise ValueError(f\"Initialization failed: {e}\") from e\n",
    "\n",
    "        \"\"\" The adams variables initiated using the valid values of wieghts and biases...\n",
    "            m - first moment,\n",
    "            v - second moment,\n",
    "            t - times step.\n",
    "            - We are changing both the biases and weights from back prop,\n",
    "             hence the two moments ...\n",
    "        \"\"\"\n",
    "        self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0 # Time step\n",
    "\n",
    "    #\n",
    "    def _get_activation(self, name):\n",
    "\n",
    "        if not isinstance(name, str):\n",
    "             raise TypeError(f\"Activation name must be a string, got {type(name)}\")\n",
    "\n",
    "        if name == 'sigmoid':\n",
    "            return self.sigmoid\n",
    "        elif name == 'relu':\n",
    "            return self.relu\n",
    "        elif name == 'leaky_relu':\n",
    "            return self.leaky_relu\n",
    "        elif name == 'softmax':\n",
    "            return self.softmax\n",
    "        elif name == 'linear':\n",
    "            return lambda x: x # No activation func applied,\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Unknown activation function: '{name}'. Valid options are 'sigmoid', 'relu', 'leaky_relu', 'softmax', 'linear'.\")\n",
    "\n",
    "    def _get_activation_derivative(self, name):\n",
    "        # Added check for name type, again...\n",
    "        if not isinstance(name, str):\n",
    "             raise TypeError(f\"Activation name must be a string, got {type(name)}\")\n",
    "\n",
    "        if name == 'sigmoid':\n",
    "            return self.sigmoid_derivative\n",
    "        elif name == 'relu':\n",
    "            return self.relu_derivative\n",
    "        elif name == 'leaky_relu':\n",
    "            return self.leaky_relu_derivative\n",
    "        elif name == 'linear':\n",
    "             return lambda x: np.ones_like(x) # Derivative of x => 1? y=mx+c\n",
    "        elif name == 'softmax':\n",
    "          ### Need to research about the derivative of this... Buggy\n",
    "             return lambda activated_output: activated_output * (1 - activated_output) # need to research abot this part more..\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function derivative for: '{name}'. Valid options are 'sigmoid', 'relu', 'leaky_relu', 'linear', 'softmax'.\")\n",
    "\n",
    "    # Definition of the activation functons..\n",
    "    def sigmoid(self, x):\n",
    "        x_clipped = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "    def sigmoid_derivative(self, activated_output):\n",
    "        return activated_output * (1 - activated_output)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, activated_output):\n",
    "        return np.where(activated_output > 0, 1, 0)\n",
    "\n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "    def leaky_relu_derivative(self, activated_output, alpha=0.01):\n",
    "        dx = np.ones_like(activated_output)\n",
    "        dx[activated_output < 0] = alpha\n",
    "        return dx\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        # Add small epsilon to prevent division by zero if all exp(x) are zero,\n",
    "        # Even thoo its unikely...\n",
    "        return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
    "\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\" Performs forward pass storing outputs and pre-activations (z values \"\"\"\n",
    "\n",
    "        if not isinstance(inputs, np.ndarray):\n",
    "            raise TypeError(f\"Input to feedForward must be a numpy array, got {type(inputs)}\")\n",
    "        if inputs.ndim == 1:\n",
    "            if inputs.shape[0] != self.input_size:\n",
    "                 raise ValueError(f\"Input sample has shape {inputs.shape} ({inputs.shape[0]} features), but network expects {self.input_size} features.\")\n",
    "            current_activation = inputs # Keep as 1D for first dot product? Let's stick to 2D internal standard\n",
    "            current_activation = current_activation.reshape(1, -1)\n",
    "\n",
    "        elif inputs.ndim == 2:\n",
    "            # Batch input, check feature dimension..\n",
    "            if inputs.shape[1] != self.input_size:\n",
    "                raise ValueError(f\"Input batch has shape {inputs.shape} ({inputs.shape[1]} features/sample), but network expects {self.input_size} features.\")\n",
    "            current_activation = inputs\n",
    "        else:\n",
    "             raise ValueError(f\"Input array must be 1D (single sample) or 2D (batch), but got ndim={inputs.ndim}\")\n",
    "\n",
    "        self.layer_inputs = [current_activation] # Store inputs (batch_size, features)\n",
    "        self.z_values = [] # Store pre-activation values (weighted sum + bias)\n",
    "\n",
    "        # (Error checking for matrix multiplication compatibility....\n",
    "        for i in range(self.num_layers - 2):\n",
    "            # Check dimensions before dot product\n",
    "            if current_activation.shape[1] != self.weights[i].shape[0]:\n",
    "                raise RuntimeError(f\"Dimension mismatch before layer {i}: Activation shape {current_activation.shape} incompatible with weight shape {self.weights[i].shape}\")\n",
    "\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            current_activation = self.activation_func(z)\n",
    "            self.layer_inputs.append(current_activation)\n",
    "\n",
    "        if current_activation.shape[1] != self.weights[-1].shape[0]:\n",
    "             raise RuntimeError(f\"Dimension mismatch before output layer: Activation shape {current_activation.shape} incompatible with weight shape {self.weights[-1].shape}\")\n",
    "\n",
    "        z_out = np.dot(current_activation, self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z_out)\n",
    "        output = self.output_activation_func(z_out)\n",
    "        self.layer_inputs.append(output) # Store final output activation(Output Node?)\n",
    "\n",
    "        # Final output shape check\n",
    "        if output.shape[1] != self.output_size:\n",
    "            raise RuntimeError(f\"Internal Error: Final output shape {output.shape} does not match network output_size {self.output_size}\")\n",
    "            # The maths i have done in here should be buggy if this error shows up...\n",
    "\n",
    "        return output\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def mean_squared_error_derivative(self, y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    # Back prop\n",
    "    def backpropagation(self, y_true, y_pred):\n",
    "        \"\"\" Performs backpropagation and calculates gradients for weights and biases. \"\"\"\n",
    "        # Check for errors in the input values parsed\n",
    "        if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n",
    "            raise TypeError(f\"y_true and y_pred must be numpy arrays, got {type(y_true)}, {type(y_pred)}\")\n",
    "        if y_true.shape != y_pred.shape:\n",
    "            raise ValueError(f\"Shape mismatch between y_true {y_true.shape} and y_pred {y_pred.shape}\")\n",
    "        if y_pred.ndim != 2: # Should be (batch_size, output_size) coming from feedForward\n",
    "             raise ValueError(f\"y_pred should be a 2D array (batch_size, output_size), got shape {y_pred.shape}\")\n",
    "        if y_pred.shape[1] != self.output_size:\n",
    "             raise ValueError(f\"y_pred second dimension ({y_pred.shape[1]}) does not match network output_size ({self.output_size})\")\n",
    "        if self.layer_inputs[-1].shape != y_pred.shape:\n",
    "             raise RuntimeError(f\"Internal state mismatch: Last layer input shape {self.layer_inputs[-1].shape} differs from y_pred shape {y_pred.shape}\")\n",
    "\n",
    "\n",
    "        # Initialize gradients For Adams....\n",
    "        grad_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_biases = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        try:\n",
    "            if self.output_activation_type in ['sigmoid', 'linear']:\n",
    "                error_derivative = self.mean_squared_error_derivative(y_true, y_pred)\n",
    "\n",
    "                deriv_output = self.output_activation_derivative(y_pred)\n",
    "                if deriv_output.shape != y_pred.shape:\n",
    "                     raise RuntimeError(f\"Derivative of output activation {self.output_activation_type} produced unexpected shape {deriv_output.shape}, expected {y_pred.shape}\")\n",
    "                output_delta = error_derivative * deriv_output\n",
    "            elif self.output_activation_type == 'softmax':\n",
    "                 # Assume Cross-Entropy Loss implicitly used in training loop / gradient calc\n",
    "                 output_delta = y_pred - y_true # This is ~ Chain rule,dE/dz directly for Cross Entroopy Loss + Softmax\n",
    "            else:\n",
    "                 raise ValueError(f\"Unsupported output activation '{self.output_activation_type}' encountered during backpropagation.\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error during output delta calculation: {e}\")\n",
    "             print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}, Output activation: {self.output_activation_type}\")\n",
    "             raise e # Re-raise after printing info\n",
    "\n",
    "        # Shape check for deltas\n",
    "        if output_delta.shape != y_pred.shape:\n",
    "             raise RuntimeError(f\"Internal Error: output_delta shape {output_delta.shape} does not match y_pred shape {y_pred.shape}\")\n",
    "\n",
    "        # --- Calculate Grads fro output nodes\n",
    "        last_hidden_activation = self.layer_inputs[-2] # Input that produced y_pred\n",
    "        if last_hidden_activation.shape[0] != output_delta.shape[0]: # Batch size check\n",
    "            raise RuntimeError(f\"Batch size mismatch: last hidden activation {last_hidden_activation.shape[0]} vs output delta {output_delta.shape[0]}\")\n",
    "        if last_hidden_activation.shape[1] != grad_weights[-1].shape[0] or output_delta.shape[1] != grad_weights[-1].shape[1]:\n",
    "            raise RuntimeError(f\"Dimension mismatch for output weights gradient: Activ {last_hidden_activation.shape}, Delta {output_delta.shape}, Expected Weight Grad {grad_weights[-1].shape}\")\n",
    "\n",
    "        grad_weights[-1] = np.dot(last_hidden_activation.T, output_delta)\n",
    "        grad_biases[-1] = np.sum(output_delta, axis=0)\n",
    "\n",
    "        # gradient shapes match parameter shapes?\n",
    "        if grad_weights[-1].shape != self.weights[-1].shape:\n",
    "             raise RuntimeError(f\"Output weight gradient shape {grad_weights[-1].shape} mismatch with weight shape {self.weights[-1].shape}\")\n",
    "        if grad_biases[-1].shape != self.biases[-1].shape:\n",
    "             raise RuntimeError(f\"Output bias gradient shape {grad_biases[-1].shape} mismatch with bias shape {self.biases[-1].shape}\")\n",
    "\n",
    "        # Propagate Error Backwards Through Hidden Layers\n",
    "        delta = output_delta\n",
    "        for i in range(self.num_layers - 2, 0, -1): # Iterate backwards from last hidden layer index (num_layers-2) down to 1\n",
    "            # Dimension checks before dot product\n",
    "            if delta.shape[1] != self.weights[i].shape[1]:\n",
    "                 raise RuntimeError(f\"Dimension mismatch backpropagating error at layer {i}: delta shape {delta.shape} vs weight shape {self.weights[i].shape}\")\n",
    "\n",
    "            error_hidden = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "            activation_h = self.layer_inputs[i]\n",
    "            # Shape check: error_hidden should match activation_h shape\n",
    "            if error_hidden.shape != activation_h.shape:\n",
    "                 raise RuntimeError(f\"Shape mismatch for hidden error: Error shape {error_hidden.shape} vs Activation shape {activation_h.shape} at layer {i}\")\n",
    "\n",
    "            # Calculate delta for this hidden layer: dE/dz_h = dE/da_h * da_h/dz_h -> Chain Rule...\n",
    "            deriv_activation_h = self.activation_derivative(activation_h)\n",
    "            if deriv_activation_h.shape != activation_h.shape:\n",
    "                 raise RuntimeError(f\"Derivative of hidden activation {self.activation_type} produced unexpected shape {deriv_activation_h.shape}, expected {activation_h.shape} at layer {i}\")\n",
    "\n",
    "            delta = error_hidden * deriv_activation_h\n",
    "            prev_layer_activation = self.layer_inputs[i-1]\n",
    "\n",
    "            # Dimension checks before dot produc\n",
    "            if prev_layer_activation.shape[0] != delta.shape[0]:\n",
    "                raise RuntimeError(f\"Batch size mismatch computing hidden grad at layer {i-1}: Activ {prev_layer_activation.shape[0]} vs Delta {delta.shape[0]}\")\n",
    "            if prev_layer_activation.shape[1] != grad_weights[i-1].shape[0] or delta.shape[1] != grad_weights[i-1].shape[1]:\n",
    "                raise RuntimeError(f\"Dimension mismatch for hidden weights gradient layer {i-1}: Activ {prev_layer_activation.shape}, Delta {delta.shape}, Expected Grad {grad_weights[i-1].shape}\")\n",
    "\n",
    "            grad_weights[i-1] = np.dot(prev_layer_activation.T, delta)\n",
    "            grad_biases[i-1] = np.sum(delta, axis=0)\n",
    "            if grad_weights[i-1].shape != self.weights[i-1].shape:\n",
    "                 raise RuntimeError(f\"Hidden weight gradient shape {grad_weights[i-1].shape} mismatch with weight shape {self.weights[i-1].shape} at layer {i-1}\")\n",
    "            if grad_biases[i-1].shape != self.biases[i-1].shape:\n",
    "                 raise RuntimeError(f\"Hidden bias gradient shape {grad_biases[i-1].shape} mismatch with bias shape {self.biases[i-1].shape} at layer {i-1}\")\n",
    "\n",
    "\n",
    "        return grad_weights, grad_biases\n",
    "\n",
    "\n",
    "    # --- Adams\n",
    "    def apply_adam_optimizer(self, grad_weights, grad_biases, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\" Updates weights and biases using Adam optimizer. \"\"\"\n",
    "        if not isinstance(grad_weights, list) or not all(isinstance(gw, np.ndarray) for gw in grad_weights):\n",
    "             raise TypeError(\"grad_weights must be a list of numpy arrays.\")\n",
    "        if not isinstance(grad_biases, list) or not all(isinstance(gb, np.ndarray) for gb in grad_biases):\n",
    "             raise TypeError(\"grad_biases must be a list of numpy arrays.\")\n",
    "        if len(grad_weights) != len(self.weights) or len(grad_biases) != len(self.biases):\n",
    "             raise ValueError(\"Number of gradient arrays does not match number of parameter arrays.\")\n",
    "        for i in range(len(self.weights)):\n",
    "             if grad_weights[i].shape != self.weights[i].shape:\n",
    "                  raise ValueError(f\"Shape mismatch for weight gradient at index {i}: got {grad_weights[i].shape}, expected {self.weights[i].shape}\")\n",
    "             if grad_biases[i].shape != self.biases[i].shape:\n",
    "                  raise ValueError(f\"Shape mismatch for bias gradient at index {i}: got {grad_biases[i].shape}, expected {self.biases[i].shape}\")\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "\n",
    "            self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * grad_weights[i]\n",
    "            self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * grad_biases[i]\n",
    "\n",
    "            self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * (grad_weights[i] ** 2)\n",
    "            self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * (grad_biases[i] ** 2)\n",
    "\n",
    "            m_hat_weights = self.m_weights[i] / (1 - beta1 ** self.t)\n",
    "            m_hat_biases = self.m_biases[i] / (1 - beta1 ** self.t)\n",
    "            v_hat_weights = self.v_weights[i] / (1 - beta2 ** self.t)\n",
    "            v_hat_biases = self.v_biases[i] / (1 - beta2 ** self.t)\n",
    "\n",
    "            self.weights[i] -= self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + epsilon)\n",
    "            self.biases[i] -= self.learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
    "\n",
    "\n",
    "    # Training Lo[]s\n",
    "    def train(self, X, y, epochs=1000, batch_size=32):\n",
    "        \"\"\" Trains the network using mini-batch gradient descent and Adam optimizer. \"\"\"\n",
    "        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
    "             raise TypeError(f\"X and y must be numpy arrays, got {type(X)}, {type(y)}\")\n",
    "        if X.ndim != 2:\n",
    "             raise ValueError(f\"Input data X must be a 2D array (samples, features), got ndim={X.ndim}\")\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "             raise ValueError(f\"Number of samples mismatch between X ({X.shape[0]}) and y ({y.shape[0]})\")\n",
    "        if X.shape[1] != self.input_size:\n",
    "             raise ValueError(f\"Input data X features ({X.shape[1]}) does not match network input_size ({self.input_size})\")\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "\n",
    "        if not isinstance(epochs, int) or epochs <= 0:\n",
    "             raise ValueError(f\"epochs must be a positive integer, got {epochs}\")\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "             raise ValueError(f\"batch_size must be a positive integer, got {batch_size}\")\n",
    "        if batch_size > num_samples:\n",
    "             print(f\"Warning: batch_size ({batch_size}) is larger than number of samples ({num_samples}). Setting batch_size to {num_samples}.\")\n",
    "             batch_size = num_samples\n",
    "\n",
    "        expected_y_dim = self.output_size\n",
    "        if y.ndim == 1:\n",
    "             if self.output_size != 1:\n",
    "                 raise ValueError(f\"Target data y is 1D, but network output_size is {self.output_size}. Reshape y or adjust network.\")\n",
    "             y = y.reshape(-1, 1) #\n",
    "        elif y.ndim == 2:\n",
    "             if y.shape[1] != self.output_size:\n",
    "                 raise ValueError(f\"Target data y has {y.shape[1]} features, but network output_size is {self.output_size}.\")\n",
    "        else:\n",
    "             raise ValueError(f\"Target data y must be 1D or 2D array, got ndim={y.ndim}\")\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(num_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation] # y is now guaranteed 2D\n",
    "\n",
    "            total_loss = 0\n",
    "            num_batches = 0 # Count actual batches processed\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, num_samples)\n",
    "                if i == end_idx: continue\n",
    "\n",
    "                X_batch = X_shuffled[i:end_idx]\n",
    "                y_batch = y_shuffled[i:end_idx]\n",
    "\n",
    "                if y_batch.shape[1] != self.output_size:\n",
    "                    raise RuntimeError(f\"Internal Error: y_batch shape {y_batch.shape} inconsistent with output_size {self.output_size}\")\n",
    "                y_pred = self.feedForward(X_batch)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    if self.output_activation_type == 'softmax':\n",
    "                         # Check if y_batch looks like one-hot encoding for softmax/CCE\n",
    "                         if not np.all((y_batch == 0) | (y_batch == 1)) or not np.all(np.sum(y_batch, axis=1) == 1):\n",
    "                              print(f\"Warning: Using Softmax/CrossEntropy loss, but y_batch doesn't appear to be one-hot- at epoch {epoch}, batch {i}.\")\n",
    "                              pass\n",
    "                         # epsilon for log stability\n",
    "                         loss = -np.mean(np.sum(y_batch * np.log(np.clip(y_pred, 1e-9, 1.0)), axis=1))\n",
    "                    elif self.output_activation_type in ['sigmoid', 'linear']:\n",
    "                        loss = self.mean_squared_error(y_batch, y_pred)\n",
    "                    else:\n",
    "                         # Scaught earlier, but for safety\n",
    "                         raise RuntimeError(f\"Unsupported output activation '{self.output_activation_type}' during loss calculation.\")\n",
    "\n",
    "                    if np.isnan(loss) or np.isinf(loss):\n",
    "                         raise ValueError(f\"Loss became NaN or Inf at epoch {epoch}, batch start {i}. Check learning rate, data scaling, or model stability.\")\n",
    "                    total_loss += loss\n",
    "                    num_batches += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during loss calculation: {e}\")\n",
    "                    print(f\"y_batch shape: {y_batch.shape}, y_pred shape: {y_pred.shape}, Loss type based on: {self.output_activation_type}\")\n",
    "                    raise e\n",
    "                grad_weights, grad_biases = self.backpropagation(y_batch, y_pred)\n",
    "                self.apply_adam_optimizer(grad_weights, grad_biases)\n",
    "            avg_loss = total_loss / num_batches if num_batches > 0 else total_loss # Avoid division by zero if dataset smaller than batch_size\n",
    "\n",
    "            if epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1: # Avoid modulo zero\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predicts output for new input data X. \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(f\"Input X must be a numpy array, got {type(X)}\")\n",
    "\n",
    "        original_ndim = X.ndim\n",
    "        if original_ndim == 1:\n",
    "            # Check shape for single sample\n",
    "            if X.shape[0] != self.input_size:\n",
    "                raise ValueError(f\"Input sample has shape {X.shape} ({X.shape[0]} features), but network expects {self.input_size} features.\")\n",
    "            X_proc = X.reshape(1, -1) # Reshape tO 2D for feedForward prop\n",
    "        elif original_ndim == 2:\n",
    "             # Check feature dimension for batch\n",
    "             if X.shape[1] != self.input_size:\n",
    "                 raise ValueError(f\"Input batch has shape {X.shape} ({X.shape[1]} features/sample), but network expects {self.input_size} features.\")\n",
    "             X_proc = X\n",
    "        else:\n",
    "             raise ValueError(f\"Input array X must be 1D (single sample) or 2D (batch), but got ndim={X.ndim}\")\n",
    "        output = self.feedForward(X_proc)\n",
    "\n",
    "        if original_ndim == 1:\n",
    "            return output.flatten()\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    # Save thie model as a file that would be loaded later on.. Sucka move here\n",
    "    def save_model(self, filename):\n",
    "        \"\"\" Saves the model's architecture and parameters using pickle. \"\"\"\n",
    "        if not isinstance(filename, str) or not filename:\n",
    "            raise ValueError(\"Filename must be a non-empty string.\")\n",
    "\n",
    "        model_data = {\n",
    "            'input_size': self.input_size, 'hidden_nodes': self.hidden_nodes, 'output_size': self.output_size,\n",
    "            'learning_rate': self.learning_rate, 'activation': self.activation_type, 'output_activation': self.output_activation_type,\n",
    "            'weights': self.weights, 'biases': self.biases,\n",
    "            'adam_state': {'m_weights': self.m_weights, 'v_weights': self.v_weights, 'm_biases': self.m_biases, 'v_biases': self.v_biases, 't': self.t}\n",
    "        }\n",
    "        try:\n",
    "            with open(filename, 'wb') as file: # Opwn the file as binary writing\n",
    "                pickle.dump(model_data, file)\n",
    "            print(f\"Model saved to {filename}\")\n",
    "        except IOError as e:\n",
    "             raise IOError(f\"Could not write model to file '{filename}': {e}\") from e\n",
    "        except pickle.PicklingError as e:\n",
    "             raise pickle.PicklingError(f\"Could not serialize model data for saving: {e}\") from e\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filename):\n",
    "        \"\"\" Loads a model from a file saved by save_model. \"\"\"\n",
    "        if not isinstance(filename, str) or not filename:\n",
    "            raise ValueError(\"Filename must be a non-empty string.\")\n",
    "        if not os.path.exists(filename):\n",
    "             raise FileNotFoundError(f\"Model file not found at '{filename}'\")\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'rb') as file:\n",
    "                model_data = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model file not found at '{filename}'\")\n",
    "        except pickle.UnpicklingError as e:\n",
    "            raise pickle.UnpicklingError(f\"Error unpickling model file '{filename}'. File might be corrupted or incompatible: {e}\") from e\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Could not read model file '{filename}': {e}\") from e\n",
    "\n",
    "        # Validate Teh Data structure of the pickle file if format is what we expect..\n",
    "        required_keys = ['input_size', 'hidden_nodes', 'output_size', 'weights', 'biases']\n",
    "        optional_keys_with_defaults = {\n",
    "            'learning_rate': 0.001, 'activation': 'relu', 'output_activation': 'sigmoid',\n",
    "            'adam_state': None\n",
    "        }\n",
    "        loaded_keys = model_data.keys()\n",
    "\n",
    "        for key in required_keys:\n",
    "            if key not in loaded_keys:\n",
    "                raise ValueError(f\"Loaded model data from '{filename}' is missing required key: '{key}'\")\n",
    "\n",
    "        # Use get with defaults for optional keys\n",
    "        init_args = {key: model_data[key] for key in required_keys}\n",
    "        for key, default in optional_keys_with_defaults.items():\n",
    "            init_args[key] = model_data.get(key, default)\n",
    "\n",
    "        try:\n",
    "             nn = cls(input_size=init_args['input_size'],\n",
    "                      hidden_nodes=init_args['hidden_nodes'],\n",
    "                      output_size=init_args['output_size'],\n",
    "                      learning_rate=init_args['learning_rate'],\n",
    "                      activation=init_args['activation'],\n",
    "                      output_activation=init_args['output_activation'])\n",
    "        except (TypeError, ValueError) as e:\n",
    "             raise ValueError(f\"Loaded parameters from '{filename}' are invalid for network initialization: {e}\") from e\n",
    "\n",
    "\n",
    "        expected_num_param_layers = len(nn.weights) # Based on loaded sizes\n",
    "        if len(model_data['weights']) != expected_num_param_layers or len(model_data['biases']) != expected_num_param_layers:\n",
    "             raise ValueError(f\"Architecture mismatch in '{filename}': Expected {expected_num_param_layers} weight/bias layers based on loaded sizes, but file contains {len(model_data['weights'])}/{len(model_data['biases'])}.\")\n",
    "\n",
    "        # Check shapes within each layer match\n",
    "        for i in range(expected_num_param_layers):\n",
    "            if not isinstance(model_data['weights'][i], np.ndarray) or model_data['weights'][i].shape != nn.weights[i].shape:\n",
    "                 raise ValueError(f\"Weight shape mismatch in layer {i} of '{filename}': Expected {nn.weights[i].shape}, file has {model_data['weights'][i].shape if isinstance(model_data['weights'][i], np.ndarray) else type(model_data['weights'][i])}\")\n",
    "            if not isinstance(model_data['biases'][i], np.ndarray) or model_data['biases'][i].shape != nn.biases[i].shape:\n",
    "                 raise ValueError(f\"Bias shape mismatch in layer {i} of '{filename}': Expected {nn.biases[i].shape}, file has {model_data['biases'][i].shape if isinstance(model_data['biases'][i], np.ndarray) else type(model_data['biases'][i])}\")\n",
    "\n",
    "        nn.weights = model_data['weights']\n",
    "        nn.biases = model_data['biases']\n",
    "\n",
    "        if init_args['adam_state'] is not None:\n",
    "             try:\n",
    "                adam_state = init_args['adam_state']\n",
    "                if isinstance(adam_state, dict) and all(k in adam_state for k in ['m_weights', 'v_weights', 'm_biases', 'v_biases', 't']):\n",
    "                     if len(adam_state['m_weights']) == expected_num_param_layers and \\\n",
    "                        len(adam_state['v_weights']) == expected_num_param_layers and \\\n",
    "                        len(adam_state['m_biases']) == expected_num_param_layers and \\\n",
    "                        len(adam_state['v_biases']) == expected_num_param_layers and \\\n",
    "                        isinstance(adam_state['t'], int):\n",
    "                          nn.m_weights = adam_state['m_weights']\n",
    "                          nn.v_weights = adam_state['v_weights']\n",
    "                          nn.m_biases = adam_state['m_biases']\n",
    "                          nn.v_biases = adam_state['v_biases']\n",
    "                          nn.t = adam_state['t']\n",
    "                          print(\"Adam optimizer state loaded.\")\n",
    "                     else:\n",
    "                          print(\"Warning: Adam state found in file but structure/size mismatch. Optimizer state not loaded.\")\n",
    "                else:\n",
    "                     print(\"Warning: Adam state found in file but format is invalid. Optimizer state not loaded.\")\n",
    "             except Exception as e:\n",
    "                  print(f\"Warning: Error loading Adam state ({e}). Optimizer state not loaded.\")\n",
    "\n",
    "\n",
    "        print(f\"Model loaded successfully from {filename}\")\n",
    "        return nn\n",
    "# Or gate training\n",
    "X_or = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "# Expected Output\n",
    "y_or = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "\n",
    "nn_or = NeuralNetwork(input_size=2, hidden_nodes=[8], output_size=1,\n",
    "                      learning_rate=0.01, activation='relu', output_activation='sigmoid')\n",
    "nn_or.train(X_or, y_or, epochs=5000, batch_size=4)\n",
    "\n",
    "predictions = nn_or.predict(X_or)\n",
    "for i in range(len(X_or)):\n",
    "    print(f\"Input: {X_or[i]}, Target: {y_or[i]}, Prediction: {predictions[i][0]:.4f} -> {int(predictions[i][0] > 0.5)}\")\n",
    "\n",
    "\n",
    "del nn_or ## Remove the instance of the Neural Network whenever done withit.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
