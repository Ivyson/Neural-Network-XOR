{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b024e3b8",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) derive their name from the convolution operation that occurs within them. This type of neural network builds upon the Multi-Layer Perceptron (MLP) model. \n",
    "\n",
    "## Why CNNs?\n",
    "The limitation of MLPs is that they only work with flattened arrays of data. For example, an image must be flattened into a one-dimensional array to be processed by an MLP. However, this flattening process often results in the loss of spatial information. Additionally, most real-world data, such as images and audio, is multi-dimensional. \n",
    "\n",
    "### Example:\n",
    "- Images are represented as matrices (e.g., a 1020x720 image has 1020 rows and 720 columns of pixels).\n",
    "- Each pixel has an RGB value, adding a third dimension to the data.\n",
    "\n",
    "Flattening such data into a single array can be computationally expensive and inefficient. CNNs address these challenges by preserving the spatial structure of the data and efficiently processing multi-dimensional inputs.\n",
    "\n",
    "## CNN Architecture\n",
    "\n",
    "CNNs are specialized neural networks designed for processing data with a grid-like topology, such as images. They consist of the following layers:\n",
    "\n",
    "### 1. **Convolutional Layers**\n",
    "- These layers apply convolutional filters (kernels) to the input data to extract local features such as edges, textures, and patterns.\n",
    "- The kernels contain weights that are learned through backpropagation, similar to the MLP model.\n",
    "\n",
    "### 2. **Pooling Layers**\n",
    "- Pooling layers perform sub-sampling or down-sampling, reducing the dimensions of the input data. This helps the network recognize objects even when they are deformed or appear in different lighting conditions.\n",
    "- **Max Pooling** is a common pooling technique. It extracts the maximum value within a selected region of the feature map.\n",
    "\n",
    "#### Example of Max Pooling:\n",
    "**Input Feature Map**:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 2 & 4 & 3 \\\\\n",
    "6 & 7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "If we perform max pooling with a stride of 2, the output feature map will be:\n",
    "\n",
    "**Output Feature Map**:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "9 & 9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### 3. **Fully Connected Layers**\n",
    "- These layers are similar to those in MLPs and are typically used for the final output.\n",
    "- They are used for tasks such as:\n",
    "    - **Classification**: Predicting categories.\n",
    "    - **Regression**: Predicting continuous values.\n",
    "    - **Probability Estimation**: Outputting probabilities for different classes.\n",
    "\n",
    "### 4. **Activation Layers**\n",
    "- Activation layers, such as ReLU (Rectified Linear Unit), introduce non-linearity into the model.\n",
    "- They can down-sample the output from previous layers into a range (e.g., 0 to 1) or compute binary values, depending on the task.\n",
    "\n",
    "\n",
    "CNNs are powerful tools for processing multi-dimensional data, especially images, and have become a cornerstone of modern deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5485006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: c:\\Users\\223146145\\Downloads\\Neural-Network-XOR\\n01443537\n",
      "Found 50 images\n",
      "Image shape: (128, 128, 3)\n",
      "Training set: 40 samples\n",
      "Testing set: 10 samples\n",
      "\n",
      "Training model...\n",
      "Found 50 images\n",
      "Image shape: (128, 128, 3)\n",
      "Training set: 40 samples\n",
      "Testing set: 10 samples\n",
      "\n",
      "Training model...\n",
      "Epoch 1/10, Loss: 0.5492\n",
      "Epoch 1/10, Loss: 0.5492\n",
      "Epoch 10/10, Loss: 0.5000\n",
      "\n",
      "Evaluating model...\n",
      "Epoch 10/10, Loss: 0.5000\n",
      "\n",
      "Evaluating model...\n",
      "Test Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'Activation._get_activation.<locals>.relu_activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1035\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1034\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfish_classifier.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel saved as fish_classifier.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# Load the model and test on a few images\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 619\u001b[39m, in \u001b[36msave_model\u001b[39m\u001b[34m(model, filename)\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03mSave the model to a file.\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m:param model: The CNN model to save.\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[33;03m:param filename: The filename to save the model to.\u001b[39;00m\n\u001b[32m    617\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'Activation._get_activation.<locals>.relu_activation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Base Layer Class for all layer types\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Forward pass - to be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # Backward pass - to be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Convolutional Layer\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        \"\"\"\n",
    "        Initialize convolutional layer\n",
    "        \n",
    "        :param input_shape: (height, width, channels)\n",
    "        :param kernel_size: Size of the convolution kernel (height, width)\n",
    "        :param depth: Number of kernels/filters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.input_height, self.input_width, self.input_channels = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Initialize filters with Xavier/Glorot initialization\n",
    "        self.kernels_shape = (kernel_size[0], kernel_size[1], self.input_channels, depth)\n",
    "        limit = np.sqrt(6 / (np.prod(kernel_size) * self.input_channels + np.prod(kernel_size) * depth))\n",
    "        self.kernels = np.random.uniform(-limit, limit, self.kernels_shape)\n",
    "        self.biases = np.zeros(depth)\n",
    "        \n",
    "        # For Adam optimizer\n",
    "        self.m_kernels = np.zeros_like(self.kernels)\n",
    "        self.v_kernels = np.zeros_like(self.kernels)\n",
    "        self.m_biases = np.zeros_like(self.biases)\n",
    "        self.v_biases = np.zeros_like(self.biases)\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        self.output_shape = (\n",
    "            self.input_height - kernel_size[0] + 1,\n",
    "            self.input_width - kernel_size[1] + 1,\n",
    "            depth\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for convolutional layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Output of shape (batch_size, new_height, new_width, depth)\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        batch_size = input_data.shape[0]\n",
    "        \n",
    "        # Initialize output array\n",
    "        self.output = np.zeros((batch_size, *self.output_shape))\n",
    "        \n",
    "        # Perform convolution for each sample in batch\n",
    "        for i in range(batch_size):\n",
    "            for d in range(self.depth):\n",
    "                for c in range(self.input_channels):\n",
    "                    # Convolve each channel with corresponding kernel\n",
    "                    self.output[i, :, :, d] += scipy.signal.convolve2d(\n",
    "                        self.input[i, :, :, c], \n",
    "                        self.kernels[:, :, c, d], \n",
    "                        mode='valid'\n",
    "                    )\n",
    "                # Add bias\n",
    "                self.output[i, :, :, d] += self.biases[d]\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \"\"\"\n",
    "        Backward pass for convolutional layer using Adam optimizer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer of shape (batch_size, height, width, depth)\n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        :param beta1: Exponential decay rate for 1st moment estimates\n",
    "        :param beta2: Exponential decay rate for 2nd moment estimates\n",
    "        :param epsilon: Small constant for numerical stability\n",
    "        :param t: Timestep (for bias correction in Adam)\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size = output_gradient.shape[0]\n",
    "        kernels_gradient = np.zeros_like(self.kernels)\n",
    "        biases_gradient = np.zeros_like(self.biases)\n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "        \n",
    "        # Calculate gradients for each sample in batch\n",
    "        for i in range(batch_size):\n",
    "            for d in range(self.depth):\n",
    "                # Gradient for biases - simple sum over height and width dimensions\n",
    "                biases_gradient[d] += np.sum(output_gradient[i, :, :, d])\n",
    "                \n",
    "                for c in range(self.input_channels):\n",
    "                    # Gradient for kernels - correlation between input and output gradient\n",
    "                    kernels_gradient[:, :, c, d] += scipy.signal.correlate2d(\n",
    "                        self.input[i, :, :, c],\n",
    "                        output_gradient[i, :, :, d],\n",
    "                        mode='valid'\n",
    "                    )\n",
    "                    \n",
    "                    # Gradient for input - full convolution with rotated kernel\n",
    "                    rotated_kernel = np.rot90(self.kernels[:, :, c, d], 2)\n",
    "                    input_gradient[i, :, :, c] += scipy.signal.convolve2d(\n",
    "                        output_gradient[i, :, :, d],\n",
    "                        rotated_kernel,\n",
    "                        mode='full'\n",
    "                    )\n",
    "        \n",
    "        # Update kernels and biases using Adam optimizer\n",
    "        # For kernels\n",
    "        self.m_kernels = beta1 * self.m_kernels + (1 - beta1) * kernels_gradient\n",
    "        self.v_kernels = beta2 * self.v_kernels + (1 - beta2) * (kernels_gradient ** 2)\n",
    "        m_hat_kernels = self.m_kernels / (1 - beta1 ** t)\n",
    "        v_hat_kernels = self.v_kernels / (1 - beta2 ** t)\n",
    "        self.kernels -= learning_rate * m_hat_kernels / (np.sqrt(v_hat_kernels) + epsilon)\n",
    "        \n",
    "        # For biases\n",
    "        self.m_biases = beta1 * self.m_biases + (1 - beta1) * biases_gradient\n",
    "        self.v_biases = beta2 * self.v_biases + (1 - beta2 ** t) * (biases_gradient ** 2)\n",
    "        m_hat_biases = self.m_biases / (1 - beta1 ** t)\n",
    "        v_hat_biases = self.v_biases / (1 - beta2 ** t)\n",
    "        self.biases -= learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# MaxPooling Layer\n",
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), stride=None):\n",
    "        \"\"\"\n",
    "        Initialize max pooling layer\n",
    "        \n",
    "        :param pool_size: Size of the pooling window (height, width)\n",
    "        :param stride: Stride of the pooling operation, defaults to pool_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "        self.max_indices = None  # To store indices of max values for backprop\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for max pooling layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Output after max pooling\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        batch_size, h_in, w_in, channels = input_data.shape\n",
    "        h_pool, w_pool = self.pool_size\n",
    "        h_stride, w_stride = self.stride\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        h_out = (h_in - h_pool) // h_stride + 1\n",
    "        w_out = (w_in - w_pool) // w_stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, h_out, w_out, channels))\n",
    "        self.max_indices = np.zeros((batch_size, h_out, w_out, channels, 2), dtype=int)\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for c in range(channels):\n",
    "                        h_start = i * h_stride\n",
    "                        h_end = h_start + h_pool\n",
    "                        w_start = j * w_stride\n",
    "                        w_end = w_start + w_pool\n",
    "                        \n",
    "                        # Get the region to pool from\n",
    "                        pool_region = input_data[b, h_start:h_end, w_start:w_end, c]\n",
    "                        \n",
    "                        # Find max value and its position within the pool region\n",
    "                        max_val = np.max(pool_region)\n",
    "                        max_pos = np.unravel_index(np.argmax(pool_region), pool_region.shape)\n",
    "                        \n",
    "                        # Store max value and its position for backprop\n",
    "                        output[b, i, j, c] = max_val\n",
    "                        self.max_indices[b, i, j, c] = max_pos\n",
    "        \n",
    "        self.output = output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for max pooling layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for pooling layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        batch_size, h_out, w_out, channels = output_gradient.shape\n",
    "        h_in, w_in = self.input.shape[1:3]\n",
    "        h_pool, w_pool = self.pool_size\n",
    "        h_stride, w_stride = self.stride\n",
    "        \n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "        \n",
    "        # Distribute gradient only to max elements\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "                    for c in range(channels):\n",
    "                        h_start = i * h_stride\n",
    "                        w_start = j * w_stride\n",
    "                        h_max, w_max = self.max_indices[b, i, j, c]\n",
    "                        \n",
    "                        # Add gradient to the position where the max was found\n",
    "                        input_gradient[b, h_start + h_max, w_start + w_max, c] += output_gradient[b, i, j, c]\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# Flatten Layer\n",
    "class Flatten(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for flatten layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, height, width, channels)\n",
    "        :return: Flattened data of shape (batch_size, height*width*channels)\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.input_shape = input_data.shape\n",
    "        batch_size = input_data.shape[0]\n",
    "        flattened_dim = np.prod(input_data.shape[1:])\n",
    "        \n",
    "        self.output = input_data.reshape(batch_size, flattened_dim)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for flatten layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for flatten layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        return output_gradient.reshape(self.input_shape)\n",
    "\n",
    "# Dense (Fully Connected) Layer\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize dense (fully connected) layer\n",
    "        \n",
    "        :param input_size: Number of input features\n",
    "        :param output_size: Number of output features\n",
    "        :param activation: Activation function ('sigmoid', 'relu', 'leaky_relu', 'linear')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "        # For Adam optimizer\n",
    "        self.m_weights = np.zeros_like(self.weights)\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.m_biases = np.zeros_like(self.biases)\n",
    "        self.v_biases = np.zeros_like(self.biases)\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activation_func = self._get_activation(self.activation_type)\n",
    "        self.activation_derivative = self._get_activation_derivative(self.activation_type)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            def sigmoid_activation(x):\n",
    "                return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            return sigmoid_activation\n",
    "        elif name == 'relu':\n",
    "            def relu_activation(x):\n",
    "                return np.maximum(0, x)\n",
    "            return relu_activation\n",
    "        elif name == 'leaky_relu':\n",
    "            def leaky_relu_activation(x):\n",
    "                return np.where(x > 0, x, x * 0.01)\n",
    "            return leaky_relu_activation\n",
    "        elif name == 'softmax':\n",
    "            def softmax_activation(x):\n",
    "                exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "                return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
    "            return softmax_activation\n",
    "        elif name == 'linear':\n",
    "            def linear_activation(x):\n",
    "                return x\n",
    "            return linear_activation\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: '{name}'\")\n",
    "    \n",
    "    def _get_activation_derivative(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            def sigmoid_derivative(x):\n",
    "                return x * (1 - x)\n",
    "            return sigmoid_derivative\n",
    "        elif name == 'relu':\n",
    "            def relu_derivative(x):\n",
    "                return np.where(x > 0, 1, 0)\n",
    "            return relu_derivative\n",
    "        elif name == 'leaky_relu':\n",
    "            def leaky_relu_derivative(x):\n",
    "                return np.where(x > 0, 1, 0.01)\n",
    "            return leaky_relu_derivative\n",
    "        elif name == 'linear':\n",
    "            def linear_derivative(x):\n",
    "                return np.ones_like(x)\n",
    "            return linear_derivative\n",
    "        elif name == 'softmax':\n",
    "            def softmax_derivative(x):\n",
    "                return x * (1 - x)  # Simplified for when used with cross-entropy\n",
    "            return softmax_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function derivative: '{name}'\")\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for dense layer\n",
    "        \n",
    "        :param input_data: Input data of shape (batch_size, input_size)\n",
    "        :return: Output after dense layer and activation\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.z = np.dot(input_data, self.weights) + self.biases\n",
    "        self.output = self.activation_func(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \"\"\"\n",
    "        Backward pass for dense layer using Adam optimizer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        :param beta1: Exponential decay rate for 1st moment estimates\n",
    "        :param beta2: Exponential decay rate for 2nd moment estimates\n",
    "        :param epsilon: Small constant for numerical stability\n",
    "        :param t: Timestep (for bias correction in Adam)\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        # Calculate gradient through activation function\n",
    "        if self.activation_type == 'softmax':\n",
    "            # Special case for softmax (assuming cross-entropy loss)\n",
    "            delta = output_gradient\n",
    "        else:\n",
    "            delta = output_gradient * self.activation_derivative(self.output)\n",
    "        \n",
    "        # Calculate gradients for weights and biases\n",
    "        weights_gradient = np.dot(self.input.T, delta)\n",
    "        biases_gradient = np.sum(delta, axis=0)\n",
    "        \n",
    "        # Calculate gradient to pass to previous layer\n",
    "        input_gradient = np.dot(delta, self.weights.T)\n",
    "        \n",
    "        # Update weights and biases using Adam optimizer\n",
    "        # For weights\n",
    "        self.m_weights = beta1 * self.m_weights + (1 - beta1) * weights_gradient\n",
    "        self.v_weights = beta2 * self.v_weights + (1 - beta2) * (weights_gradient ** 2)\n",
    "        m_hat_weights = self.m_weights / (1 - beta1 ** t)\n",
    "        v_hat_weights = self.v_weights / (1 - beta2 ** t)\n",
    "        self.weights -= learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + epsilon)\n",
    "        \n",
    "        # For biases\n",
    "        self.m_biases = beta1 * self.m_biases + (1 - beta1) * biases_gradient\n",
    "        self.v_biases = beta2 * self.v_biases + (1 - beta2 ** t) * (biases_gradient ** 2)\n",
    "        m_hat_biases = self.m_biases / (1 - beta1 ** t)\n",
    "        v_hat_biases = self.v_biases / (1 - beta2 ** t)\n",
    "        self.biases -= learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "# Activation Layer as a separate layer\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation):\n",
    "        \"\"\"\n",
    "        Initialize activation layer\n",
    "        \n",
    "        :param activation: Activation function name\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation_type = activation\n",
    "        self.activation_func = self._get_activation(activation)\n",
    "        self.activation_derivative = self._get_activation_derivative(activation)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            def sigmoid_activation(x):\n",
    "                return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            return sigmoid_activation\n",
    "        elif name == 'relu':\n",
    "            def relu_activation(x):\n",
    "                return np.maximum(0, x)\n",
    "            return relu_activation\n",
    "        elif name == 'leaky_relu':\n",
    "            def leaky_relu_activation(x):\n",
    "                return np.where(x > 0, x, x * 0.01)\n",
    "            return leaky_relu_activation\n",
    "        elif name == 'softmax':\n",
    "            def softmax_activation(x):\n",
    "                exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "                return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
    "            return softmax_activation\n",
    "        elif name == 'linear':\n",
    "            def linear_activation(x):\n",
    "                return x\n",
    "            return linear_activation\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: '{name}'\")\n",
    "    \n",
    "    def _get_activation_derivative(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            def sigmoid_derivative(x):\n",
    "                return x * (1 - x)\n",
    "            return sigmoid_derivative\n",
    "        elif name == 'relu':\n",
    "            def relu_derivative(x):\n",
    "                return np.where(x > 0, 1, 0)\n",
    "            return relu_derivative\n",
    "        elif name == 'leaky_relu':\n",
    "            def leaky_relu_derivative(x):\n",
    "                return np.where(x > 0, 1, 0.01)\n",
    "            return leaky_relu_derivative\n",
    "        elif name == 'linear':\n",
    "            def linear_derivative(x):\n",
    "                return np.ones_like(x)\n",
    "            return linear_derivative\n",
    "        elif name == 'softmax':\n",
    "            def softmax_derivative(x):\n",
    "                return x * (1 - x)  # Simplified for when used with cross-entropy\n",
    "            return softmax_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function derivative: '{name}'\")\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass for activation layer\n",
    "        \n",
    "        :param input_data: Input data\n",
    "        :return: Output after activation\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = self.activation_func(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Backward pass for activation layer\n",
    "        \n",
    "        :param output_gradient: Gradient from next layer\n",
    "        :param learning_rate: Not used for activation layer\n",
    "        :return: Gradient with respect to input\n",
    "        \"\"\"\n",
    "        if self.activation_type == 'softmax':\n",
    "            # Special case for softmax (assuming cross-entropy loss)\n",
    "            return output_gradient\n",
    "        return output_gradient * self.activation_derivative(self.output)\n",
    "\n",
    "# CNN Model\n",
    "class CNN:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize CNN model\n",
    "        \n",
    "        :param learning_rate: Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0  # Time step for Adam optimizer\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Add a layer to the model\n",
    "        \n",
    "        :param layer: Layer to add\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Make predictions with the model\n",
    "        \n",
    "        :param input_data: Input data\n",
    "        :return: Model predictions\n",
    "        \"\"\"\n",
    "        # Ensure input data is in batch format\n",
    "        if input_data.ndim == 3:  # Single image (height, width, channels)\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        :param X_train: Training data\n",
    "        :param y_train: Training labels\n",
    "        :param epochs: Number of epochs\n",
    "        :param batch_size: Batch size\n",
    "        :param verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        # Ensure X_train is in batch format\n",
    "        if X_train.ndim == 3:  # Single image (height, width, channels)\n",
    "            X_train = np.expand_dims(X_train, axis=0)\n",
    "        \n",
    "        # Ensure y_train is in batch format\n",
    "        if y_train.ndim == 1:  \n",
    "            y_train = np.expand_dims(y_train, axis=0)\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "            # Train in batches\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                # Get batch\n",
    "                X_batch = X_shuffled[i:min(i + batch_size, n_samples)]\n",
    "                y_batch = y_shuffled[i:min(i + batch_size, n_samples)]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.predict(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss += self._compute_loss(y_batch, output)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.t += 1  # Increment time step for Adam optimizer\n",
    "                grad = self._compute_loss_gradient(y_batch, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad, self.learning_rate, t=self.t)\n",
    "            \n",
    "            # Average loss over all batches\n",
    "            loss /= n_samples / batch_size\n",
    "            \n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        :param y_true: True labels\n",
    "        :param y_pred: Predicted labels\n",
    "        :return: Loss value\n",
    "        \"\"\"\n",
    "        # Mean Squared Error\n",
    "        return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))\n",
    "    \n",
    "    def _compute_loss_gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss\n",
    "        \n",
    "        :param y_true: True labels\n",
    "        :param y_pred: Predicted labels\n",
    "        :return: Gradient of loss\n",
    "        \"\"\"\n",
    "        # Gradient of Mean Squared Error\n",
    "        return 2 * (y_pred - y_true) / y_true.shape[0]\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save model to file\n",
    "        :param filename: Filename to save to\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filename):\n",
    "        \"\"\"\n",
    "        Load model from file\n",
    "        \n",
    "        :param filename: Filename to load from\n",
    "        :return: Loaded model\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "    :param model: The CNN model to train.\n",
    "    :param X_train: Training data.\n",
    "    :param y_train: Training labels.\n",
    "    :param epochs: Number of epochs to train for.\n",
    "    :param batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    model.train(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "def test_model(model, image_path, target_size):\n",
    "    \"\"\"\n",
    "    Test the model on a single image.\n",
    "    :param model: The CNN model to test.\n",
    "    :param image_path: Path to the image to test.\n",
    "    :param target_size: Tuple specifying the target size for resizing the image.\n",
    "    :return: The model's prediction.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f'Could not load image from {image_path}')\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    prediction = model.predict(img)\n",
    "    return prediction\n",
    "\n",
    "def load_and_preprocess_images(image_folder, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from a folder.\n",
    "    :param image_folder: Path to the folder containing images.\n",
    "    :param target_size: Tuple specifying the target size for resizing the images.\n",
    "    :return: Tuple of (images, labels).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\"Error: Folder {image_folder} does not exist\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Check if this is a folder directly contains images\n",
    "    contents = os.listdir(image_folder)\n",
    "    has_subfolders = any(os.path.isdir(os.path.join(image_folder, item)) for item in contents)\n",
    "    \n",
    "    if has_subfolders:\n",
    "        # Process folder with class subfolders\n",
    "        class_names = [item for item in contents if os.path.isdir(os.path.join(image_folder, item))]\n",
    "        class_to_index = {class_name: idx for idx, class_name in enumerate(sorted(class_names))}\n",
    "        \n",
    "        for class_name in class_names:\n",
    "            class_folder = os.path.join(image_folder, class_name)\n",
    "            class_idx = class_to_index[class_name]\n",
    "            \n",
    "            for filename in os.listdir(class_folder):\n",
    "                if filename.lower().endswith(('.jpeg', '.jpg', '.png', '.bmp', '.gif')):\n",
    "                    image_path = os.path.join(class_folder, filename)\n",
    "                    img = cv2.imread(image_path)\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Could not load image {filename}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if len(img.shape) == 2:  # If grayscale, convert to RGB\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    else:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "                    \n",
    "                    img = cv2.resize(img, target_size)\n",
    "                    img = img.astype(np.float32) / 255.0\n",
    "                    images.append(img)\n",
    "                    labels.append(class_idx)\n",
    "    else:\n",
    "        # Process folder with images directly (single class)\n",
    "        for filename in contents:\n",
    "            if filename.lower().endswith(('.jpeg', '.jpg', '.png', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(image_folder, filename)\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is None:\n",
    "                    print(f\"Warning: Could not load image {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                if len(img.shape) == 2:  # If grayscale, convert to RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                else:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "                \n",
    "                img = cv2.resize(img, target_size)\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                images.append(img)\n",
    "                labels.append(0)  # Single class, assign label 0\n",
    "    \n",
    "    if not images:\n",
    "        print(\"No valid images found in the folder.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def train_model_with_images(model, image_folder, test_size=0.2, epochs=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the CNN model using images from a folder.\n",
    "    :param model: The CNN model to train.\n",
    "    :param image_folder: Path to the folder containing images.\n",
    "    :param test_size: Proportion of the dataset to include in the test split.\n",
    "    :param epochs: Number of epochs to train for.\n",
    "    :param batch_size: Batch size for training.\n",
    "    \"\"\"\n",
    "    # Load and preprocess images\n",
    "    X, y = load_and_preprocess_images(image_folder)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"No images found for training\")\n",
    "        return\n",
    "        \n",
    "    # Dynamically determine input size from the first image\n",
    "    input_shape = X.shape[1:]  # (height, width, channels)\n",
    "    \n",
    "    # Count unique classes and prepare appropriate labels format\n",
    "    unique_classes = np.unique(y)\n",
    "    num_classes = len(unique_classes)\n",
    "    \n",
    "    print(f\"Found {num_classes} unique classes in the dataset\")\n",
    "    \n",
    "    # Format labels appropriately\n",
    "    # For binary classification (sigmoid output)\n",
    "    if num_classes <= 2:\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "    # For multi-class classification (softmax output)\n",
    "    else:\n",
    "        # One-hot encode the labels\n",
    "        y_onehot = np.zeros((len(y), num_classes))\n",
    "        for i, label in enumerate(y):\n",
    "            y_onehot[i, label] = 1\n",
    "        y = y_onehot\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy based on classification type\n",
    "    if num_classes <= 2:\n",
    "        # Binary classification\n",
    "        pred_classes = (predictions > 0.5).astype(int)\n",
    "        accuracy = np.mean(pred_classes == y_test)\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(pred_classes == true_classes)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_dynamic_cnn(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Create a CNN model dynamically based on input shape\n",
    "    \n",
    "    :param input_shape: Input shape (height, width, channels)\n",
    "    :param num_classes: Number of classes for classification\n",
    "    :return: CNN model\n",
    "    \"\"\"\n",
    "    model = CNN(learning_rate=0.001)\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(input_shape=input_shape, kernel_size=(3, 3), depth=16))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # MaxPooling layer\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    h, w, c = input_shape\n",
    "    # Calculate dimensions after first conv and pool\n",
    "    new_h = (h - 3 + 1) // 2\n",
    "    new_w = (w - 3 + 1) // 2\n",
    "    model.add(Conv2D(input_shape=(new_h, new_w, 16), kernel_size=(3, 3), depth=32))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # MaxPooling layer\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Calculate input size for dense layer\n",
    "    dense_input_size = 32 * ((new_h - 3 + 1) // 2) * ((new_w - 3 + 1) // 2)\n",
    "    \n",
    "    # Dense layer\n",
    "    model.add(Dense(dense_input_size, 128, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 1:\n",
    "        model.add(Dense(128, 1, activation='sigmoid'))  # Binary classification\n",
    "    else:\n",
    "        model.add(Dense(128, num_classes, activation='softmax'))  # Multi-class classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test_model_on_images(model, test_folder, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Test the model on a folder of images.\n",
    "    \n",
    "    :param model: The CNN model to test.\n",
    "    :param test_folder: Path to the folder containing test images.\n",
    "    :param target_size: Tuple specifying the target size for resizing the images.\n",
    "    :return: Dictionary with filenames as keys and predictions as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if not os.path.exists(test_folder):\n",
    "        print(f\"Error: Test folder {test_folder} does not exist\")\n",
    "        return results\n",
    "    \n",
    "    # Get all image files in the test folder\n",
    "    files = [f for f in os.listdir(test_folder) if f.lower().endswith(('.jpeg', '.jpg', '.png', '.bmp', '.gif'))]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No image files found in the test folder\")\n",
    "        return results\n",
    "    \n",
    "    for filename in files:\n",
    "        image_path = os.path.join(test_folder, filename)\n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load image {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Preprocess image\n",
    "        if len(img.shape) == 2:  # If grayscale, convert to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        \n",
    "        img = cv2.resize(img, target_size)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "        \n",
    "        # Store result\n",
    "        results[filename] = prediction\n",
    "        \n",
    "        # Print result for this image\n",
    "        print(f\"Image: {filename}\")\n",
    "        \n",
    "        # Handle binary vs multi-class prediction\n",
    "        if len(prediction) == 1:\n",
    "            # Binary classification\n",
    "            pred_class = 1 if prediction[0] > 0.5 else 0\n",
    "            confidence = prediction[0] if pred_class == 1 else 1 - prediction[0]\n",
    "            print(f\"  Predicted class: {pred_class} with confidence: {confidence*100:.2f}%\")\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            pred_class = np.argmax(prediction)\n",
    "            confidence = prediction[pred_class]\n",
    "            print(f\"  Predicted class: {pred_class} with confidence: {confidence*100:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_predictions(model, images, true_labels, num_samples=5, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize model predictions with matplotlib\n",
    "    \n",
    "    :param model: The trained CNN model\n",
    "    :param images: Array of images\n",
    "    :param true_labels: Array of true labels\n",
    "    :param num_samples: Number of samples to visualize\n",
    "    :param figsize: Figure size for the plot\n",
    "    \"\"\"\n",
    "    # Select a random subset of images\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    indices = np.random.choice(len(images), num_samples, replace=False)\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=figsize)\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]  # Make axes iterable if only one sample\n",
    "    \n",
    "    # Plot each image with its prediction\n",
    "    for i, idx in enumerate(indices):\n",
    "        img = images[idx]\n",
    "        true_label = true_labels[idx][0] if true_labels[idx].shape == (1,) else true_labels[idx]\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "        \n",
    "        # Determine prediction class and confidence\n",
    "        if len(pred) == 1:  # Binary classification\n",
    "            pred_class = 1 if pred[0] > 0.5 else 0\n",
    "            confidence = pred[0] if pred_class == 1 else 1 - pred[0]\n",
    "        else:  # Multi-class classification\n",
    "            pred_class = np.argmax(pred)\n",
    "            confidence = pred[pred_class]\n",
    "        \n",
    "        # Convert from RGB back to BGR for display (optional)\n",
    "        display_img = img.copy()\n",
    "        \n",
    "        # Add a colored border based on prediction correctness\n",
    "        if (len(true_label) == 1 and pred_class == true_label[0]) or (len(true_label) > 1 and pred_class == np.argmax(true_label)):\n",
    "            # Green border for correct predictions\n",
    "            bordered_img = cv2.copyMakeBorder(display_img, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=(0, 1, 0))\n",
    "        else:\n",
    "            # Red border for incorrect predictions\n",
    "            bordered_img = cv2.copyMakeBorder(display_img, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=(1, 0, 0))\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i].imshow(bordered_img)\n",
    "        axes[i].set_title(f\"True: {true_label[0] if len(true_label.shape) > 0 else true_label}\\nPred: {pred_class}\\nConf: {confidence:.2f}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_with_visualization(model, test_images, test_labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Test model and visualize results\n",
    "    \n",
    "    :param model: The trained CNN model\n",
    "    :param test_images: Array of test images\n",
    "    :param test_labels: Array of test labels\n",
    "    :param num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    # Evaluate on all test data\n",
    "    predictions = model.predict(test_images)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if predictions.shape[1] == 1:  # Binary classification\n",
    "        pred_classes = (predictions > 0.5).astype(int)\n",
    "        true_classes = test_labels\n",
    "        accuracy = np.mean(pred_classes == true_classes)\n",
    "    else:  # Multi-class classification\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(test_labels, axis=1) if test_labels.shape[1] > 1 else test_labels\n",
    "        accuracy = np.mean(pred_classes == true_classes)\n",
    "    \n",
    "    print(f\"Overall Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_predictions(model, test_images, test_labels, num_samples)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the folder containing images\n",
    "    image_folder = r\"c:\\Users\\223146145\\Downloads\\Neural-Network-XOR\\n01443537\" # Need to change this as soon as i use Google colab..\n",
    "    print(f\"Loading images from: {image_folder}\")\n",
    "    \n",
    "    # First load all images to get dimensions and count classes\n",
    "    sample_images, sample_labels = load_and_preprocess_images(image_folder, target_size=(128, 128))\n",
    "    \n",
    "    if len(sample_images) == 0:\n",
    "        print(\"Error: No images found. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(sample_images)} images\")\n",
    "        input_shape = sample_images[0].shape\n",
    "        print(f\"Image shape: {input_shape}\")\n",
    "        \n",
    "        # Since we're using a single folder with all the same class images, assign half to class 0 and half to class 1\n",
    "        # just to create a binary classification scenario for testing\n",
    "        \n",
    "        # Divide images into two \"virtual\" classes for demonstration\n",
    "        half_idx = len(sample_images) // 2\n",
    "        labels = np.zeros(len(sample_images))\n",
    "        labels[half_idx:] = 1  # Second half gets label 1\n",
    "        \n",
    "        # Prepare datasets with balanced classes\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            sample_images, labels, test_size=0.2, random_state=42, stratify=labels \n",
    "        )\n",
    "        \n",
    "        # Format labels for binary classification\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Create a binary classification CNN model\n",
    "        model = create_dynamic_cnn(input_shape, num_classes=1)\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nTraining model...\")\n",
    "        model.train(X_train, y_train, epochs=10, batch_size=4)\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        predictions = model.predict(X_test)\n",
    "        pred_classes = (predictions > 0.5).astype(int)\n",
    "        accuracy = np.mean(pred_classes == y_test)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        # Save the model\n",
    "        model.save_model(model, \"fish_classifier.pkl\")\n",
    "        print(\"Model saved as fish_classifier.pkl\")\n",
    "        \n",
    "        # Load the model and test on a few images\n",
    "        print(\"\\nLoading model and testing on sample images...\")\n",
    "        loaded_model = model.load_model(\"fish_classifier.pkl\")\n",
    "        \n",
    "        # Test on a few random images\n",
    "        num_test = min(5, len(X_test))\n",
    "        for i in range(num_test):\n",
    "            test_img = X_test[i]\n",
    "            true_label = y_test[i][0]\n",
    "            \n",
    "            pred = model.predict(np.expand_dims(test_img, axis=0))[0][0] \n",
    "            pred_class = 1 if pred > 0.5 else 0\n",
    "            \n",
    "            print(f\"Sample {i+1}: True class={true_label}, Predicted class={pred_class}, Confidence={pred*100:.2f}% if class 1, {(1-pred)*100:.2f}% if class 0\")\n",
    "        \n",
    "        # Visualize predictions\n",
    "        # print(\"\\nVisualizing predictions...\")\n",
    "        # test_with_visualization(model, X_test, y_test, num_samples=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
