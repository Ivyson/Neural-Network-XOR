{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivyson/Neural-Network-XOR/blob/main/Main_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EEG Signal Processing with Neural Networks\n",
        "\n",
        "This notebook builds upon a previously designed Neural Network system\n",
        "(`Model.ipynb`) to dive down to the fundamentals of the EEG signal Processing\n",
        "\n",
        "\n",
        "Introduced Major error checking schemes for safety. This is to ensure that the model functions accurately and the data manipulation processes in within the model are as accurate as possible.\n",
        "## Considerations for EEG Neural Networks\n",
        "\n",
        "1. **Data Characteristics:**\n",
        " - EEG data consists of time series recordings of electrical brain activity.\n",
        " - Data is measured from multiple electrodes placed on the scalp.\n",
        " - The sampling rate of the recording determines the *temporal resolution*.\n",
        "\n",
        "2. **Data Preprocessing and Feature Extraction:**\n",
        "  - **Labeling:** Ensure your training data is accurately labeled for the task.\n",
        "  - **Feature Engineering:** Explore relevant EEG features like power spectral density,\n",
        "wavelet coefficients, or statistical measures (mean, variance, etc.).\n",
        " - **Artifact Removal:** Consider techniques to mitigate noise and artifacts in the EEG signal.\n",
        "\n",
        "3. **Neural Network Architecture:**\n",
        " - **Activation Functions:** Utilize ReLU or parameterized ReLU to enhance performance\n",
        "and mitigate the vanishing gradient problem.\n",
        "- **Output Layer:** The number of output neurons should correspond to the desired application.\n",
        "For binary classification (e.g., seizure detection), a single output neuron with a sigmoid\n",
        "activation is suitable. For multi-class problems, use an appropriate number of output\n",
        "neurons with a softmax activation.\n",
        "- **Network Types:** Explore Convolutional Neural Networks (CNNs), Recurrent Neural Networks\n",
        "(RNNs), or Long Short-Term Memory (LSTM) networks, depending on the specific task.\n",
        "\n",
        "4. **Loss Function and Optimization:**\n",
        "- **Loss Function:** Use categorical cross-entropy for multi-class problems (with softmax) or\n",
        "binary cross-entropy for binary classification.\n",
        "- **Optimizer:** Consider Adam optimizer for efficient training.  \n",
        "\n",
        "## Adam Optimizer\n",
        "\n",
        "The Adam optimizer combines momentum and RMSprop to adjust learning rates for individual parameters.\n",
        "It helps in finding the global minimum and avoids getting stuck in local minima.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "**Initialization:**\n",
        "-  $m_t = 0$ (First Moment Vector)\n",
        "-  $v_t = 0$ (Second Moment Vector)\n",
        "-  $t = 0$ (Time step)\n",
        "\n",
        "**Update Rules:**\n",
        "1. Compute the gradient: $\\nabla \\theta_t$\n",
        "2. Update first moment: $m_t = \\beta_1 \\times m_{t-1} + (1 - \\beta_1) \\times \\nabla \\theta_t$\n",
        "3. Update second moment: $v_t = \\beta_2 \\times v_{t-1} + (1 - \\beta_2) \\times (\\nabla \\theta_t)^2$\n",
        "4. Calculate bias-corrected first moment: $\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$\n",
        "5. Calculate bias-corrected second moment: $\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}$\n",
        "6. Update parameters: $\\theta_t = \\theta_{t-1} - \\alpha \\times \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}$\n",
        "\n",
        "Where:\n",
        "- $\\beta_1$, $\\beta_2$: Exponential decay rates for moment estimates (typically 0.9 and 0.999)\n",
        "- $\\alpha$: Learning rate\n",
        "- $\\epsilon$: Small constant to prevent division by zero (e.g., 1e-8)\n",
        "\n",
        "\n",
        "## Resources Used\n",
        "- [MIT LECTURE](https://www.youtube.com/watch?v=wrEcHhoJxjM)\n"
      ],
      "metadata": {
        "id": "vGVQbazyw3AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "### What are Features ?\n",
        "Features are the descriptions of a dataset, that is the segmentation of the dataset which describes the different aspects of the data which will be used to determine a specific outcome. Example includes, the geolocational aspect of the house which determines the pricing of the house.\n",
        "\n",
        "To better describe data, sufficient features should be available, this allows for better predicting. For an example, when perfoming audio sampling, the audio signal is broken down up until it has 40 dimensions\n",
        "\n",
        "Due to the complexity of the nueral network, the architecture has to change. The flexibility of allowing the user to explicitly input the activation function that should be used between the hidden layers and the activation function that should be used between the last hidden layer and the output layer. This is to accomodate Probabilistic outcomes(using softmax), Binary outcome(using sigmoids) etc...\n",
        "\n",
        "Start using a different approach for initalizing the weights of the nueral network, This is because, when randomly selecting weights, the random function may generate small weights values which might cause the nueral network to take time to learn the pattern in the dataset, or not even learn anything due to the vanishing weights/ gradients. Sometimes, the random function can generate too large weights, which will cause the network to jump around the local minimum, making the network not learn anything useful at all, or having an exploding gradient. Use **Xavier** method to get the weights, which uses number of the input nodes and output nodes to carefully scale the initial weights.\n",
        "### Xavier Method\n",
        "Works Best with sigmoid function\n",
        "1. **Xavier Normal Distribution**\n",
        "\n",
        "Initialises the weights using the following approach:\n",
        "$\\text{W}_{\\text{ij}} = \\text{Normal Distribution}(0, \\sigma)$\n",
        "\n",
        "  Whereby:\n",
        "  \n",
        "  $\\sigma = \\sqrt{ \\frac{2}{N_{in}+N_{out}}}$\n",
        "\n",
        "  $0 \\rightarrow \\text{Mean Value}$\n",
        "2. **Xavier Uniform Distribution**\n",
        "\n",
        "Initialises the weights using the following approach:\n",
        "  $\\text{W}_{ij} = \\text{Uniform Distribution}\\left[\\sqrt{-\\frac{6}{N_{in}+N_{out}}},\\sqrt{\\frac{6}{N_{in}+N_{out}}}\\right]$\n",
        "\n",
        "  Whereby:\n",
        "\n",
        "  $N_{in} \\rightarrow \\text{the number of weights going into one node}$\n",
        "  \n",
        "  $N_{out} \\rightarrow \\text{the number of weights going out of the same node}$\n",
        "\n",
        "### HE init\n",
        "Normally good when used with ReLU activation function\n",
        "1. **He Int Normal Destribution**\n",
        "Uses the following approach to initiate the weights:\n",
        "\n",
        "$\\text{W}_{\\text{ij}} = \\text{Normal Distribution}(0, \\sigma)$\n",
        "\n",
        "  Whereby:\n",
        "  \n",
        "  $\\sigma = \\sqrt{ \\frac{2}{N_{in}}}$\n",
        "\n",
        "  $0 \\rightarrow \\text{Mean Value}$\n",
        "\n",
        "2. **He Int Uniform Destribution**\n",
        "Uses the following approach to initiate the weights:\n",
        "\n",
        "$\\text{W}_{ij} = \\text{Uniform Distribution}\\left[\\sqrt{-\\frac{6}{N_{in}}},\\sqrt{\\frac{6}{N_{in}+}}\\right]$"
      ],
      "metadata": {
        "id": "W_DZcwh28nN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvaUE5pX1CqK",
        "outputId": "93177224-fffc-4fa4-f976-c0764c943a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxiq25cJwyWf",
        "outputId": "2b3a5861-5fb1-450e-a5d0-fd4ab226612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2201\n",
            "Epoch 500, Loss: 0.0030\n",
            "Epoch 1000, Loss: 0.0008\n",
            "Epoch 1500, Loss: 0.0004\n",
            "Epoch 2000, Loss: 0.0002\n",
            "Epoch 2500, Loss: 0.0001\n",
            "Epoch 3000, Loss: 0.0001\n",
            "Epoch 3500, Loss: 0.0001\n",
            "Epoch 4000, Loss: 0.0000\n",
            "Epoch 4500, Loss: 0.0000\n",
            "Epoch 4999, Loss: 0.0000\n",
            "Input: [0 0], Target: [0], Prediction: 0.0098 -> 0\n",
            "Input: [0 1], Target: [1], Prediction: 0.9992 -> 1\n",
            "Input: [1 0], Target: [1], Prediction: 0.9983 -> 1\n",
            "Input: [1 1], Target: [1], Prediction: 1.0000 -> 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import scipy.signal\n",
        "import pickle\n",
        "import os\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_nodes: np.ndarray,\n",
        "                 output_size: np.ndarray,\n",
        "                 learning_rate: Union[int, float] = 0.001, # This can be a float or an int, even though an int is not something i recommend.(I am a good engineer)\n",
        "                 activation:str = 'relu',\n",
        "                 output_activation:str ='sigmoid'):\n",
        "        \"\"\"\n",
        "        Initializes the Neural Network with error checking for parameters.\n",
        "\n",
        "        :param input_size: Number of input features (must be positive integer)\n",
        "        :param hidden_nodes: Integer or List specifying number of neurons in each hidden layer (each must be positive integer)\n",
        "        :param output_size: Number of output neurons (must be positive integer)\n",
        "        :param learning_rate: Learning rate for optimizer (must be positive float)\n",
        "        :param activation: Activation function for hidden layers ('sigmoid', 'relu', 'leaky_relu', 'linear')\n",
        "        :param output_activation: Activation function for the output layer ('sigmoid', 'softmax', 'linear')\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If input types are incorrect.\n",
        "            ValueError: If input values are invalid (e.g., non-positive sizes, invalid activation names).\n",
        "        \"\"\"\n",
        "        # Input Validation\n",
        "        if not isinstance(input_size, int) or input_size <= 0:\n",
        "            raise ValueError(f\"input_size must be a positive integer, got {input_size}\")\n",
        "        if not isinstance(output_size, int) or output_size <= 0:\n",
        "            raise ValueError(f\"output_size must be a positive integer, got {output_size}\")\n",
        "        if not isinstance(learning_rate, (float, int)) or learning_rate <= 0:\n",
        "            raise ValueError(f\"learning_rate must be a positive number, got {learning_rate}\")\n",
        "        if not isinstance(activation, str):\n",
        "             raise TypeError(f\"activation must be a string, got {type(activation)}\")\n",
        "        if not isinstance(output_activation, str):\n",
        "             raise TypeError(f\"output_activation must be a string, got {type(output_activation)}\")\n",
        "\n",
        "        # alidate Hidden_nodes content\n",
        "        if isinstance(hidden_nodes, int):\n",
        "            if hidden_nodes <= 0:\n",
        "                 raise ValueError(f\"If hidden_nodes is an integer, it must be positive, got {hidden_nodes}\")\n",
        "            processed_hidden_nodes = [hidden_nodes] # convert single int to list\n",
        "        elif isinstance(hidden_nodes, list):\n",
        "            if not all(isinstance(n, int) and n > 0 for n in hidden_nodes):\n",
        "                 raise ValueError(f\"If hidden_nodes is a list, all elements must be positive integers, got {hidden_nodes}\")\n",
        "            processed_hidden_nodes = hidden_nodes # a list already..\n",
        "        else:\n",
        "            raise TypeError(f\"hidden_nodes must be a positive integer or a list of positive integers, got {type(hidden_nodes)}\")\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_nodes = processed_hidden_nodes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = float(learning_rate)\n",
        "        self.activation_type = activation\n",
        "        self.output_activation_type = output_activation\n",
        "\n",
        "        # This part is safe now, the checks above have saved it...\n",
        "        layer_sizes = [self.input_size] + self.hidden_nodes + [self.output_size]\n",
        "        self.num_layers = len(layer_sizes)\n",
        "\n",
        "        # Init weights and\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Layer sizes are guaranteed positive ints here\n",
        "            # Now Check for potential division by zero\n",
        "            fan_in = layer_sizes[i]\n",
        "            fan_out = layer_sizes[i+1]\n",
        "            limit = np.sqrt(6 / (fan_in + fan_out)) # Use Xavier Method.. Safer..\n",
        "\n",
        "            self.weights.append(np.random.uniform(-limit, limit, (fan_in, fan_out)))\n",
        "            self.biases.append(np.zeros(fan_out))\n",
        "\n",
        "        try:\n",
        "            self.activation_func = self._get_activation(self.activation_type)\n",
        "            self.activation_derivative = self._get_activation_derivative(self.activation_type)\n",
        "            self.output_activation_func = self._get_activation(self.output_activation_type)\n",
        "            self.output_activation_derivative = self._get_activation_derivative(self.output_activation_type)\n",
        "        except ValueError as e:\n",
        "             raise ValueError(f\"Initialization failed: {e}\") from e\n",
        "\n",
        "        \"\"\" The adams variables initiated using the valid values of wieghts and biases...\n",
        "            m - first moment,\n",
        "            v - second moment,\n",
        "            t - times step.\n",
        "            - We are changing both the biases and weights from back prop,\n",
        "             hence the two moments ...\n",
        "        \"\"\"\n",
        "        self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        self.t = 0 # Time step\n",
        "\n",
        "    #\n",
        "    def _get_activation(self, name):\n",
        "\n",
        "        if not isinstance(name, str):\n",
        "             raise TypeError(f\"Activation name must be a string, got {type(name)}\")\n",
        "\n",
        "        if name == 'sigmoid':\n",
        "            return self.sigmoid\n",
        "        elif name == 'relu':\n",
        "            return self.relu\n",
        "        elif name == 'leaky_relu':\n",
        "            return self.leaky_relu\n",
        "        elif name == 'softmax':\n",
        "            return self.softmax\n",
        "        elif name == 'linear':\n",
        "            return lambda x: x # No activation func applied,\n",
        "        else:\n",
        "\n",
        "            raise ValueError(f\"Unknown activation function: '{name}'. Valid options are 'sigmoid', 'relu', 'leaky_relu', 'softmax', 'linear'.\")\n",
        "\n",
        "    def _get_activation_derivative(self, name):\n",
        "        # Added check for name type, again...\n",
        "        if not isinstance(name, str):\n",
        "             raise TypeError(f\"Activation name must be a string, got {type(name)}\")\n",
        "\n",
        "        if name == 'sigmoid':\n",
        "            return self.sigmoid_derivative\n",
        "        elif name == 'relu':\n",
        "            return self.relu_derivative\n",
        "        elif name == 'leaky_relu':\n",
        "            return self.leaky_relu_derivative\n",
        "        elif name == 'linear':\n",
        "             return lambda x: np.ones_like(x) # Derivative of x => 1? y=mx+c\n",
        "        elif name == 'softmax':\n",
        "          ### Need to research about the derivative of this... Buggy\n",
        "             return lambda activated_output: activated_output * (1 - activated_output) # need to research abot this part more..\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function derivative for: '{name}'. Valid options are 'sigmoid', 'relu', 'leaky_relu', 'linear', 'softmax'.\")\n",
        "\n",
        "    # Definition of the activation functons..\n",
        "    def sigmoid(self, x):\n",
        "        x_clipped = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x_clipped))\n",
        "\n",
        "    def sigmoid_derivative(self, activated_output):\n",
        "        return activated_output * (1 - activated_output)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, activated_output):\n",
        "        return np.where(activated_output > 0, 1, 0)\n",
        "\n",
        "    def leaky_relu(self, x, alpha=0.01):\n",
        "        return np.where(x > 0, x, x * alpha)\n",
        "\n",
        "    def leaky_relu_derivative(self, activated_output, alpha=0.01):\n",
        "        dx = np.ones_like(activated_output)\n",
        "        dx[activated_output < 0] = alpha\n",
        "        return dx\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        # Add small epsilon to prevent division by zero if all exp(x) are zero,\n",
        "        # Even thoo its unikely...\n",
        "        return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)\n",
        "\n",
        "\n",
        "    def feedForward(self, inputs):\n",
        "        \"\"\" Performs forward pass storing outputs and pre-activations (z values \"\"\"\n",
        "\n",
        "        if not isinstance(inputs, np.ndarray):\n",
        "            raise TypeError(f\"Input to feedForward must be a numpy array, got {type(inputs)}\")\n",
        "        if inputs.ndim == 1:\n",
        "            if inputs.shape[0] != self.input_size:\n",
        "                 raise ValueError(f\"Input sample has shape {inputs.shape} ({inputs.shape[0]} features), but network expects {self.input_size} features.\")\n",
        "            current_activation = inputs # Keep as 1D for first dot product? Let's stick to 2D internal standard\n",
        "            current_activation = current_activation.reshape(1, -1)\n",
        "\n",
        "        elif inputs.ndim == 2:\n",
        "            # Batch input, check feature dimension..\n",
        "            if inputs.shape[1] != self.input_size:\n",
        "                raise ValueError(f\"Input batch has shape {inputs.shape} ({inputs.shape[1]} features/sample), but network expects {self.input_size} features.\")\n",
        "            current_activation = inputs\n",
        "        else:\n",
        "             raise ValueError(f\"Input array must be 1D (single sample) or 2D (batch), but got ndim={inputs.ndim}\")\n",
        "\n",
        "        self.layer_inputs = [current_activation] # Store inputs (batch_size, features)\n",
        "        self.z_values = [] # Store pre-activation values (weighted sum + bias)\n",
        "\n",
        "        # (Error checking for matrix multiplication compatibility....\n",
        "        for i in range(self.num_layers - 2):\n",
        "            # Check dimensions before dot product\n",
        "            if current_activation.shape[1] != self.weights[i].shape[0]:\n",
        "                raise RuntimeError(f\"Dimension mismatch before layer {i}: Activation shape {current_activation.shape} incompatible with weight shape {self.weights[i].shape}\")\n",
        "\n",
        "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            current_activation = self.activation_func(z)\n",
        "            self.layer_inputs.append(current_activation)\n",
        "\n",
        "        if current_activation.shape[1] != self.weights[-1].shape[0]:\n",
        "             raise RuntimeError(f\"Dimension mismatch before output layer: Activation shape {current_activation.shape} incompatible with weight shape {self.weights[-1].shape}\")\n",
        "\n",
        "        z_out = np.dot(current_activation, self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z_out)\n",
        "        output = self.output_activation_func(z_out)\n",
        "        self.layer_inputs.append(output) # Store final output activation(Output Node?)\n",
        "\n",
        "        # Final output shape check\n",
        "        if output.shape[1] != self.output_size:\n",
        "            raise RuntimeError(f\"Internal Error: Final output shape {output.shape} does not match network output_size {self.output_size}\")\n",
        "            # The maths i have done in here should be buggy if this error shows up...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def mean_squared_error(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    def mean_squared_error_derivative(self, y_true, y_pred):\n",
        "        return y_pred - y_true\n",
        "\n",
        "    # Back prop\n",
        "    def backpropagation(self, y_true, y_pred):\n",
        "        \"\"\" Performs backpropagation and calculates gradients for weights and biases. \"\"\"\n",
        "        # Check for errors in the input values parsed\n",
        "        if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n",
        "            raise TypeError(f\"y_true and y_pred must be numpy arrays, got {type(y_true)}, {type(y_pred)}\")\n",
        "        if y_true.shape != y_pred.shape:\n",
        "            raise ValueError(f\"Shape mismatch between y_true {y_true.shape} and y_pred {y_pred.shape}\")\n",
        "        if y_pred.ndim != 2: # Should be (batch_size, output_size) coming from feedForward\n",
        "             raise ValueError(f\"y_pred should be a 2D array (batch_size, output_size), got shape {y_pred.shape}\")\n",
        "        if y_pred.shape[1] != self.output_size:\n",
        "             raise ValueError(f\"y_pred second dimension ({y_pred.shape[1]}) does not match network output_size ({self.output_size})\")\n",
        "        if self.layer_inputs[-1].shape != y_pred.shape:\n",
        "             raise RuntimeError(f\"Internal state mismatch: Last layer input shape {self.layer_inputs[-1].shape} differs from y_pred shape {y_pred.shape}\")\n",
        "\n",
        "\n",
        "        # Initialize gradients For Adams....\n",
        "        grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        try:\n",
        "            if self.output_activation_type in ['sigmoid', 'linear']:\n",
        "                error_derivative = self.mean_squared_error_derivative(y_true, y_pred)\n",
        "\n",
        "                deriv_output = self.output_activation_derivative(y_pred)\n",
        "                if deriv_output.shape != y_pred.shape:\n",
        "                     raise RuntimeError(f\"Derivative of output activation {self.output_activation_type} produced unexpected shape {deriv_output.shape}, expected {y_pred.shape}\")\n",
        "                output_delta = error_derivative * deriv_output\n",
        "            elif self.output_activation_type == 'softmax':\n",
        "                 # Assume Cross-Entropy Loss implicitly used in training loop / gradient calc\n",
        "                 output_delta = y_pred - y_true # This is ~ Chain rule,dE/dz directly for Cross Entroopy Loss + Softmax\n",
        "            else:\n",
        "                 raise ValueError(f\"Unsupported output activation '{self.output_activation_type}' encountered during backpropagation.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error during output delta calculation: {e}\")\n",
        "             print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}, Output activation: {self.output_activation_type}\")\n",
        "             raise e # Re-raise after printing info\n",
        "\n",
        "        # Shape check for deltas\n",
        "        if output_delta.shape != y_pred.shape:\n",
        "             raise RuntimeError(f\"Internal Error: output_delta shape {output_delta.shape} does not match y_pred shape {y_pred.shape}\")\n",
        "\n",
        "        # --- Calculate Grads fro output nodes\n",
        "        last_hidden_activation = self.layer_inputs[-2] # Input that produced y_pred\n",
        "        if last_hidden_activation.shape[0] != output_delta.shape[0]: # Batch size check\n",
        "            raise RuntimeError(f\"Batch size mismatch: last hidden activation {last_hidden_activation.shape[0]} vs output delta {output_delta.shape[0]}\")\n",
        "        if last_hidden_activation.shape[1] != grad_weights[-1].shape[0] or output_delta.shape[1] != grad_weights[-1].shape[1]:\n",
        "            raise RuntimeError(f\"Dimension mismatch for output weights gradient: Activ {last_hidden_activation.shape}, Delta {output_delta.shape}, Expected Weight Grad {grad_weights[-1].shape}\")\n",
        "\n",
        "        grad_weights[-1] = np.dot(last_hidden_activation.T, output_delta)\n",
        "        grad_biases[-1] = np.sum(output_delta, axis=0)\n",
        "\n",
        "        # gradient shapes match parameter shapes?\n",
        "        if grad_weights[-1].shape != self.weights[-1].shape:\n",
        "             raise RuntimeError(f\"Output weight gradient shape {grad_weights[-1].shape} mismatch with weight shape {self.weights[-1].shape}\")\n",
        "        if grad_biases[-1].shape != self.biases[-1].shape:\n",
        "             raise RuntimeError(f\"Output bias gradient shape {grad_biases[-1].shape} mismatch with bias shape {self.biases[-1].shape}\")\n",
        "\n",
        "        # Propagate Error Backwards Through Hidden Layers\n",
        "        delta = output_delta\n",
        "        for i in range(self.num_layers - 2, 0, -1): # Iterate backwards from last hidden layer index (num_layers-2) down to 1\n",
        "            # Dimension checks before dot product\n",
        "            if delta.shape[1] != self.weights[i].shape[1]:\n",
        "                 raise RuntimeError(f\"Dimension mismatch backpropagating error at layer {i}: delta shape {delta.shape} vs weight shape {self.weights[i].shape}\")\n",
        "\n",
        "            error_hidden = np.dot(delta, self.weights[i].T)\n",
        "\n",
        "            activation_h = self.layer_inputs[i]\n",
        "            # Shape check: error_hidden should match activation_h shape\n",
        "            if error_hidden.shape != activation_h.shape:\n",
        "                 raise RuntimeError(f\"Shape mismatch for hidden error: Error shape {error_hidden.shape} vs Activation shape {activation_h.shape} at layer {i}\")\n",
        "\n",
        "            # Calculate delta for this hidden layer: dE/dz_h = dE/da_h * da_h/dz_h -> Chain Rule...\n",
        "            deriv_activation_h = self.activation_derivative(activation_h)\n",
        "            if deriv_activation_h.shape != activation_h.shape:\n",
        "                 raise RuntimeError(f\"Derivative of hidden activation {self.activation_type} produced unexpected shape {deriv_activation_h.shape}, expected {activation_h.shape} at layer {i}\")\n",
        "\n",
        "            delta = error_hidden * deriv_activation_h\n",
        "            prev_layer_activation = self.layer_inputs[i-1]\n",
        "\n",
        "            # Dimension checks before dot produc\n",
        "            if prev_layer_activation.shape[0] != delta.shape[0]:\n",
        "                raise RuntimeError(f\"Batch size mismatch computing hidden grad at layer {i-1}: Activ {prev_layer_activation.shape[0]} vs Delta {delta.shape[0]}\")\n",
        "            if prev_layer_activation.shape[1] != grad_weights[i-1].shape[0] or delta.shape[1] != grad_weights[i-1].shape[1]:\n",
        "                raise RuntimeError(f\"Dimension mismatch for hidden weights gradient layer {i-1}: Activ {prev_layer_activation.shape}, Delta {delta.shape}, Expected Grad {grad_weights[i-1].shape}\")\n",
        "\n",
        "            grad_weights[i-1] = np.dot(prev_layer_activation.T, delta)\n",
        "            grad_biases[i-1] = np.sum(delta, axis=0)\n",
        "            if grad_weights[i-1].shape != self.weights[i-1].shape:\n",
        "                 raise RuntimeError(f\"Hidden weight gradient shape {grad_weights[i-1].shape} mismatch with weight shape {self.weights[i-1].shape} at layer {i-1}\")\n",
        "            if grad_biases[i-1].shape != self.biases[i-1].shape:\n",
        "                 raise RuntimeError(f\"Hidden bias gradient shape {grad_biases[i-1].shape} mismatch with bias shape {self.biases[i-1].shape} at layer {i-1}\")\n",
        "\n",
        "\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "\n",
        "    # --- Adams\n",
        "    def apply_adam_optimizer(self, grad_weights, grad_biases, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        \"\"\" Updates weights and biases using Adam optimizer. \"\"\"\n",
        "        if not isinstance(grad_weights, list) or not all(isinstance(gw, np.ndarray) for gw in grad_weights):\n",
        "             raise TypeError(\"grad_weights must be a list of numpy arrays.\")\n",
        "        if not isinstance(grad_biases, list) or not all(isinstance(gb, np.ndarray) for gb in grad_biases):\n",
        "             raise TypeError(\"grad_biases must be a list of numpy arrays.\")\n",
        "        if len(grad_weights) != len(self.weights) or len(grad_biases) != len(self.biases):\n",
        "             raise ValueError(\"Number of gradient arrays does not match number of parameter arrays.\")\n",
        "        for i in range(len(self.weights)):\n",
        "             if grad_weights[i].shape != self.weights[i].shape:\n",
        "                  raise ValueError(f\"Shape mismatch for weight gradient at index {i}: got {grad_weights[i].shape}, expected {self.weights[i].shape}\")\n",
        "             if grad_biases[i].shape != self.biases[i].shape:\n",
        "                  raise ValueError(f\"Shape mismatch for bias gradient at index {i}: got {grad_biases[i].shape}, expected {self.biases[i].shape}\")\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "\n",
        "            self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * grad_weights[i]\n",
        "            self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * grad_biases[i]\n",
        "\n",
        "            self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * (grad_weights[i] ** 2)\n",
        "            self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * (grad_biases[i] ** 2)\n",
        "\n",
        "            m_hat_weights = self.m_weights[i] / (1 - beta1 ** self.t)\n",
        "            m_hat_biases = self.m_biases[i] / (1 - beta1 ** self.t)\n",
        "            v_hat_weights = self.v_weights[i] / (1 - beta2 ** self.t)\n",
        "            v_hat_biases = self.v_biases[i] / (1 - beta2 ** self.t)\n",
        "\n",
        "            self.weights[i] -= self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + epsilon)\n",
        "            self.biases[i] -= self.learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + epsilon)\n",
        "\n",
        "\n",
        "    # Training Lo[]s\n",
        "    def train(self, X, y, epochs=1000, batch_size=32):\n",
        "        \"\"\" Trains the network using mini-batch gradient descent and Adam optimizer. \"\"\"\n",
        "        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n",
        "             raise TypeError(f\"X and y must be numpy arrays, got {type(X)}, {type(y)}\")\n",
        "        if X.ndim != 2:\n",
        "             raise ValueError(f\"Input data X must be a 2D array (samples, features), got ndim={X.ndim}\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "             raise ValueError(f\"Number of samples mismatch between X ({X.shape[0]}) and y ({y.shape[0]})\")\n",
        "        if X.shape[1] != self.input_size:\n",
        "             raise ValueError(f\"Input data X features ({X.shape[1]}) does not match network input_size ({self.input_size})\")\n",
        "\n",
        "        num_samples = X.shape[0]\n",
        "\n",
        "        if not isinstance(epochs, int) or epochs <= 0:\n",
        "             raise ValueError(f\"epochs must be a positive integer, got {epochs}\")\n",
        "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
        "             raise ValueError(f\"batch_size must be a positive integer, got {batch_size}\")\n",
        "        if batch_size > num_samples:\n",
        "             print(f\"Warning: batch_size ({batch_size}) is larger than number of samples ({num_samples}). Setting batch_size to {num_samples}.\")\n",
        "             batch_size = num_samples\n",
        "\n",
        "        expected_y_dim = self.output_size\n",
        "        if y.ndim == 1:\n",
        "             if self.output_size != 1:\n",
        "                 raise ValueError(f\"Target data y is 1D, but network output_size is {self.output_size}. Reshape y or adjust network.\")\n",
        "             y = y.reshape(-1, 1) #\n",
        "        elif y.ndim == 2:\n",
        "             if y.shape[1] != self.output_size:\n",
        "                 raise ValueError(f\"Target data y has {y.shape[1]} features, but network output_size is {self.output_size}.\")\n",
        "        else:\n",
        "             raise ValueError(f\"Target data y must be 1D or 2D array, got ndim={y.ndim}\")\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            permutation = np.random.permutation(num_samples)\n",
        "            X_shuffled = X[permutation]\n",
        "            y_shuffled = y[permutation] # y is now guaranteed 2D\n",
        "\n",
        "            total_loss = 0\n",
        "            num_batches = 0 # Count actual batches processed\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                end_idx = min(i + batch_size, num_samples)\n",
        "                if i == end_idx: continue\n",
        "\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "\n",
        "                if y_batch.shape[1] != self.output_size:\n",
        "                    raise RuntimeError(f\"Internal Error: y_batch shape {y_batch.shape} inconsistent with output_size {self.output_size}\")\n",
        "                y_pred = self.feedForward(X_batch)\n",
        "\n",
        "\n",
        "                try:\n",
        "                    if self.output_activation_type == 'softmax':\n",
        "                         # Check if y_batch looks like one-hot encoding for softmax/CCE\n",
        "                         if not np.all((y_batch == 0) | (y_batch == 1)) or not np.all(np.sum(y_batch, axis=1) == 1):\n",
        "                              print(f\"Warning: Using Softmax/CrossEntropy loss, but y_batch doesn't appear to be one-hot- at epoch {epoch}, batch {i}.\")\n",
        "                              pass\n",
        "                         # epsilon for log stability\n",
        "                         loss = -np.mean(np.sum(y_batch * np.log(np.clip(y_pred, 1e-9, 1.0)), axis=1))\n",
        "                    elif self.output_activation_type in ['sigmoid', 'linear']:\n",
        "                        loss = self.mean_squared_error(y_batch, y_pred)\n",
        "                    else:\n",
        "                         # Scaught earlier, but for safety\n",
        "                         raise RuntimeError(f\"Unsupported output activation '{self.output_activation_type}' during loss calculation.\")\n",
        "\n",
        "                    if np.isnan(loss) or np.isinf(loss):\n",
        "                         raise ValueError(f\"Loss became NaN or Inf at epoch {epoch}, batch start {i}. Check learning rate, data scaling, or model stability.\")\n",
        "                    total_loss += loss\n",
        "                    num_batches += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during loss calculation: {e}\")\n",
        "                    print(f\"y_batch shape: {y_batch.shape}, y_pred shape: {y_pred.shape}, Loss type based on: {self.output_activation_type}\")\n",
        "                    raise e\n",
        "                grad_weights, grad_biases = self.backpropagation(y_batch, y_pred)\n",
        "                self.apply_adam_optimizer(grad_weights, grad_biases)\n",
        "            avg_loss = total_loss / num_batches if num_batches > 0 else total_loss # Avoid division by zero if dataset smaller than batch_size\n",
        "\n",
        "            if epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1: # Avoid modulo zero\n",
        "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predicts output for new input data X. \"\"\"\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            raise TypeError(f\"Input X must be a numpy array, got {type(X)}\")\n",
        "\n",
        "        original_ndim = X.ndim\n",
        "        if original_ndim == 1:\n",
        "            # Check shape for single sample\n",
        "            if X.shape[0] != self.input_size:\n",
        "                raise ValueError(f\"Input sample has shape {X.shape} ({X.shape[0]} features), but network expects {self.input_size} features.\")\n",
        "            X_proc = X.reshape(1, -1) # Reshape tO 2D for feedForward prop\n",
        "        elif original_ndim == 2:\n",
        "             # Check feature dimension for batch\n",
        "             if X.shape[1] != self.input_size:\n",
        "                 raise ValueError(f\"Input batch has shape {X.shape} ({X.shape[1]} features/sample), but network expects {self.input_size} features.\")\n",
        "             X_proc = X\n",
        "        else:\n",
        "             raise ValueError(f\"Input array X must be 1D (single sample) or 2D (batch), but got ndim={X.ndim}\")\n",
        "        output = self.feedForward(X_proc)\n",
        "\n",
        "        if original_ndim == 1:\n",
        "            return output.flatten()\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "    # Save thie model as a file that would be loaded later on.. Sucka move here\n",
        "    def save_model(self, filename):\n",
        "        \"\"\" Saves the model's architecture and parameters using pickle. \"\"\"\n",
        "        if not isinstance(filename, str) or not filename:\n",
        "            raise ValueError(\"Filename must be a non-empty string.\")\n",
        "\n",
        "        model_data = {\n",
        "            'input_size': self.input_size, 'hidden_nodes': self.hidden_nodes, 'output_size': self.output_size,\n",
        "            'learning_rate': self.learning_rate, 'activation': self.activation_type, 'output_activation': self.output_activation_type,\n",
        "            'weights': self.weights, 'biases': self.biases,\n",
        "            'adam_state': {'m_weights': self.m_weights, 'v_weights': self.v_weights, 'm_biases': self.m_biases, 'v_biases': self.v_biases, 't': self.t}\n",
        "        }\n",
        "        try:\n",
        "            with open(filename, 'wb') as file: # Opwn the file as binary writing\n",
        "                pickle.dump(model_data, file)\n",
        "            print(f\"Model saved to {filename}\")\n",
        "        except IOError as e:\n",
        "             raise IOError(f\"Could not write model to file '{filename}': {e}\") from e\n",
        "        except pickle.PicklingError as e:\n",
        "             raise pickle.PicklingError(f\"Could not serialize model data for saving: {e}\") from e\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filename):\n",
        "        \"\"\" Loads a model from a file saved by save_model. \"\"\"\n",
        "        if not isinstance(filename, str) or not filename:\n",
        "            raise ValueError(\"Filename must be a non-empty string.\")\n",
        "        if not os.path.exists(filename):\n",
        "             raise FileNotFoundError(f\"Model file not found at '{filename}'\")\n",
        "\n",
        "        try:\n",
        "            with open(filename, 'rb') as file:\n",
        "                model_data = pickle.load(file)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Model file not found at '{filename}'\")\n",
        "        except pickle.UnpicklingError as e:\n",
        "            raise pickle.UnpicklingError(f\"Error unpickling model file '{filename}'. File might be corrupted or incompatible: {e}\") from e\n",
        "        except IOError as e:\n",
        "            raise IOError(f\"Could not read model file '{filename}': {e}\") from e\n",
        "\n",
        "        # Validate Teh Data structure of the pickle file if format is what we expect..\n",
        "        required_keys = ['input_size', 'hidden_nodes', 'output_size', 'weights', 'biases']\n",
        "        optional_keys_with_defaults = {\n",
        "            'learning_rate': 0.001, 'activation': 'relu', 'output_activation': 'sigmoid',\n",
        "            'adam_state': None\n",
        "        }\n",
        "        loaded_keys = model_data.keys()\n",
        "\n",
        "        for key in required_keys:\n",
        "            if key not in loaded_keys:\n",
        "                raise ValueError(f\"Loaded model data from '{filename}' is missing required key: '{key}'\")\n",
        "\n",
        "        # Use get with defaults for optional keys\n",
        "        init_args = {key: model_data[key] for key in required_keys}\n",
        "        for key, default in optional_keys_with_defaults.items():\n",
        "            init_args[key] = model_data.get(key, default)\n",
        "\n",
        "        try:\n",
        "             nn = cls(input_size=init_args['input_size'],\n",
        "                      hidden_nodes=init_args['hidden_nodes'],\n",
        "                      output_size=init_args['output_size'],\n",
        "                      learning_rate=init_args['learning_rate'],\n",
        "                      activation=init_args['activation'],\n",
        "                      output_activation=init_args['output_activation'])\n",
        "        except (TypeError, ValueError) as e:\n",
        "             raise ValueError(f\"Loaded parameters from '{filename}' are invalid for network initialization: {e}\") from e\n",
        "\n",
        "\n",
        "        expected_num_param_layers = len(nn.weights) # Based on loaded sizes\n",
        "        if len(model_data['weights']) != expected_num_param_layers or len(model_data['biases']) != expected_num_param_layers:\n",
        "             raise ValueError(f\"Architecture mismatch in '{filename}': Expected {expected_num_param_layers} weight/bias layers based on loaded sizes, but file contains {len(model_data['weights'])}/{len(model_data['biases'])}.\")\n",
        "\n",
        "        # Check shapes within each layer match\n",
        "        for i in range(expected_num_param_layers):\n",
        "            if not isinstance(model_data['weights'][i], np.ndarray) or model_data['weights'][i].shape != nn.weights[i].shape:\n",
        "                 raise ValueError(f\"Weight shape mismatch in layer {i} of '{filename}': Expected {nn.weights[i].shape}, file has {model_data['weights'][i].shape if isinstance(model_data['weights'][i], np.ndarray) else type(model_data['weights'][i])}\")\n",
        "            if not isinstance(model_data['biases'][i], np.ndarray) or model_data['biases'][i].shape != nn.biases[i].shape:\n",
        "                 raise ValueError(f\"Bias shape mismatch in layer {i} of '{filename}': Expected {nn.biases[i].shape}, file has {model_data['biases'][i].shape if isinstance(model_data['biases'][i], np.ndarray) else type(model_data['biases'][i])}\")\n",
        "\n",
        "        nn.weights = model_data['weights']\n",
        "        nn.biases = model_data['biases']\n",
        "\n",
        "        if init_args['adam_state'] is not None:\n",
        "             try:\n",
        "                adam_state = init_args['adam_state']\n",
        "                if isinstance(adam_state, dict) and all(k in adam_state for k in ['m_weights', 'v_weights', 'm_biases', 'v_biases', 't']):\n",
        "                     if len(adam_state['m_weights']) == expected_num_param_layers and \\\n",
        "                        len(adam_state['v_weights']) == expected_num_param_layers and \\\n",
        "                        len(adam_state['m_biases']) == expected_num_param_layers and \\\n",
        "                        len(adam_state['v_biases']) == expected_num_param_layers and \\\n",
        "                        isinstance(adam_state['t'], int):\n",
        "                          nn.m_weights = adam_state['m_weights']\n",
        "                          nn.v_weights = adam_state['v_weights']\n",
        "                          nn.m_biases = adam_state['m_biases']\n",
        "                          nn.v_biases = adam_state['v_biases']\n",
        "                          nn.t = adam_state['t']\n",
        "                          print(\"Adam optimizer state loaded.\")\n",
        "                     else:\n",
        "                          print(\"Warning: Adam state found in file but structure/size mismatch. Optimizer state not loaded.\")\n",
        "                else:\n",
        "                     print(\"Warning: Adam state found in file but format is invalid. Optimizer state not loaded.\")\n",
        "             except Exception as e:\n",
        "                  print(f\"Warning: Error loading Adam state ({e}). Optimizer state not loaded.\")\n",
        "\n",
        "\n",
        "        print(f\"Model loaded successfully from {filename}\")\n",
        "        return nn\n",
        "# Or gate training\n",
        "X_or = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "# Expected Output\n",
        "y_or = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "\n",
        "\n",
        "nn_or = NeuralNetwork(input_size=2, hidden_nodes=[8], output_size=1,\n",
        "                      learning_rate=0.01, activation='relu', output_activation='sigmoid')\n",
        "nn_or.train(X_or, y_or, epochs=5000, batch_size=4)\n",
        "\n",
        "predictions = nn_or.predict(X_or)\n",
        "for i in range(len(X_or)):\n",
        "    print(f\"Input: {X_or[i]}, Target: {y_or[i]}, Prediction: {predictions[i][0]:.4f} -> {int(predictions[i][0] > 0.5)}\")\n",
        "\n",
        "\n",
        "del nn_or ## Remove the instance of the Neural Network whenever done withit.."
      ]
    }
  ]
}