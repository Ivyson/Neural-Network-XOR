{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivyson/Neural-Network-XOR/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpq-VL8HcwOU",
        "outputId": "2b5a3bcd-5226-4cab-ba0e-d509589ad669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.62\n",
            "Epoch 3000, Loss: 0.02\n",
            "Epoch 6000, Loss: 0.01\n",
            "Epoch 9000, Loss: 0.01\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, input_size, hidden_nodes, output_size, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        :param input_size: Number of input neurons\n",
        "        :param hidden_nodes: List specifying number of neurons in each hidden layer\n",
        "        :param output_size: Number of output neurons\n",
        "        :param learning_rate: Learning rate for weight updates\n",
        "\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_nodes = hidden_nodes  # List specifying neurons per hidden layer\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Define the architecture: input layer → hidden layers → output layer\n",
        "        layer_sizes = [input_size] + hidden_nodes + [output_size]\n",
        "\n",
        "        # Initialize weights and biases dynamically\n",
        "        self.weights = [np.random.rand(layer_sizes[i], layer_sizes[i+1]) - 0.5 for i in range(len(layer_sizes) - 1)]\n",
        "        self.biases = [np.random.rand(layer_sizes[i+1]) - 0.5 for i in range(len(layer_sizes) - 1)]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedForward(self, inputs):\n",
        "        # Forward propagation through all layers.......\n",
        "        self.layers = [inputs]  # Store activations of all layers\n",
        "        for i in range(len(self.weights)):\n",
        "            inputs = self.sigmoid(np.dot(inputs, self.weights[i]) + self.biases[i])\n",
        "            self.layers.append(inputs)  # Save outputs of [i+1] layer for backpropagation\n",
        "        return inputs\n",
        "\n",
        "\n",
        "    # Back Prop, to update the weights and Biases of the nueral network\n",
        "    def backpropagation(self, target_output):\n",
        "        errors = [target_output - self.layers[-1]]  # Output layer error\n",
        "        deltas = [errors[0] * self.sigmoid_derivative(self.layers[-1])]  # Output layer delta\n",
        "\n",
        "        # Get Dltas For each hidden layer in reverse order\n",
        "        for i in range(len(self.hidden_nodes), 0, -1):\n",
        "            errors.insert(0, np.dot(deltas[0], self.weights[i].T))  # Error of previous layer\n",
        "            deltas.insert(0, errors[0] * self.sigmoid_derivative(self.layers[i]))  # Delta, previous layer\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += np.dot(self.layers[i].reshape(-1, 1), deltas[i].reshape(1, -1)) * self.learning_rate\n",
        "            self.biases[i] += deltas[i] * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs=10000):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(len(X)):\n",
        "                self.feedForward(X[i])\n",
        "                self.backpropagation(y[i])\n",
        "                total_loss += np.sum(np.abs(y[i] - self.layers[-1]))\n",
        "\n",
        "            if epoch % 3000 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {(total_loss / len(X)):.2f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.feedForward(x) for x in X]\n",
        "\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        with open(filename, 'wb') as file:\n",
        "            np.save(file, self.input_size)\n",
        "            np.save(file, self.hidden_nodes)\n",
        "            np.save(file, self.output_size)\n",
        "\n",
        "\n",
        "            # Save all weights and biases\n",
        "            for weight in self.weights:\n",
        "                np.save(file, weight)\n",
        "\n",
        "            for bias in self.biases:\n",
        "                np.save(file, bias)\n",
        "\n",
        "\n",
        "    def Load_Model(self, filename):\n",
        "      # Open the file in read mode\n",
        "      with open(filename, 'rb') as file:\n",
        "          self.input_size = np.load(file)\n",
        "          self.hidden_nodes = np.load(file)\n",
        "          self.output_size = np.load(file)\n",
        "          # Size of the weights = [len(inputs)*Hidden[0]][Hidden[0]*]\n",
        "          self.weights = []\n",
        "          self.biases = []\n",
        "          size = [self.input_size] + self.hidden_nodes + [self.output_size]\n",
        "          self.weights = [np.load(file) for _ in range(len(size) + 1)]\n",
        "          self.biases = [np.load(file) for _ in range(len(size) + 1)]\n",
        "          # print(f'Biases : {self.biases}')\n",
        "          # print(f'Weights : {self.weights}')\n",
        "          # print(f'Input Size : {self.input_size}')\n",
        "          # print(f'Hidden Nodes : {self.hidden_nodes}')\n",
        "          # print(f'Output Size : {self.output_size} ')\n",
        "          print(f'Model : {filename} has been Loaded successfully')\n",
        "\n",
        "\n",
        "# OR dataset\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "#  Desired Output/ Target Output\n",
        "y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "\n",
        "# Create A Nueral Network with 2 inputs, and 2 Hidden Layers with nodes each,\n",
        "nn = NeuralNetwork(input_size=2, hidden_nodes=[20], output_size=1, learning_rate=0.2)\n",
        "nn.train(X, y, epochs=10000)\n",
        "\n",
        "\"\"\"\n",
        "The Model is too small and the learning rate is pretty quick\n",
        "So, The 50 Thousands epochs are not tha much of a deal,\n",
        "ever since the model has to solve a basic problem\n",
        "\n",
        "\"\"\"\n",
        "nn.save_model('model.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Load_Model('model.txt')"
      ],
      "metadata": {
        "id": "u3cWuOEXPWEi"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}