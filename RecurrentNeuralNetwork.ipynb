{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXQEeHcdtonNJQgWq64nGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivyson/Neural-Network-XOR/blob/main/RecurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "This session builds upon the progress we made with the **Multi-Layered Perceptron (MLP)** neural network [linked here](https://github.com/Ivyson/Neural-Network-XOR/blob/main/Model.ipynb).\n",
        "## What is this?\n",
        "\n",
        "To explain the concept of **Recurrent Neural Networks (RNNs)**, we'll begin with the **unrolled version** to build intuition step by step. Once the structure is clear, we can abstract it using higher-level blocks to represent the full architecture.\n",
        "\n",
        "To understand RNNs, we first revisit the **Simple Perceptron** - a small network unit that receives an input and performs a weighted sum followed by an activation function:\n",
        "\n",
        "$$\n",
        "y_t = f(W \\cdot X + b)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $W$ is the weight matrix (or vector in 1-layer cases),\n",
        "* $X$ is the input vector,\n",
        "* $b$ is the bias term,\n",
        "* $f$ is a non-linear activation function, typically one of the following:\n",
        "\n",
        "**Activation Functions**:\n",
        "\n",
        "* **Sigmoid**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ $\\rightarrow$ Range: $[0, 1]$\n",
        "* **ReLU**: $f(x) = \\max(0, x)$\n",
        "* **Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ $\\rightarrow$ Range: $[–1, 1]$\n",
        "\n",
        "These functions introduce non-linearity, allowing the network to learn more complex patterns that linear models cannot.\n",
        "\n",
        "### Why Do We Need Recurrent Networks?\n",
        "\n",
        "In **MLPs**, each input is processed independently. The model sees a snapshot of the input, passes it through hidden layers, and produces an output. This is sufficient for problems where inputs are static. However, many real-world problem, like language modeling(Translation, or word completion), audio processing(from a previous musical note, predict the next note), or time series forecasting(This will be explored in future for EEG-Project) - involve **sequences**, where the current output depends on previous inputs.\n",
        "\n",
        "This is where **Recurrent Neural Networks** come in. RNNs allow the network to **retain memory** by passing information from previous steps to future ones.\n",
        "\n",
        "### The Unrolled View\n",
        "\n",
        "To understand how RNNs work, imagine taking a simple perceptron (one that processes input $\\rightarrow$ weighted sum $\\rightarrow$ activation) and **stacking multiple copies** side by side. Each copy handles a different time step in a sequence. This is shown clearly in the diagram below:\n",
        "\n",
        "![Unrolled RNN diagram](https://images.deepai.org/converted-papers/1501.00299/24.png)\n",
        "\n",
        "Each node labeled $h_{t-n}$ represents the same RNN unit at different time steps. But unlike in an MLP, each unit receives **two inputs**:\n",
        "\n",
        "* the current input $x_t$\n",
        "* and a **hidden state** $h_{t-1}$, passed from the previous time step.\n",
        "\n",
        "This hidden state acts like a **memory**, allowing the network to learn temporal(time depended) dependencies across steps.\n",
        "\n",
        "### The Math Behind the Memory\n",
        "\n",
        "At each time step $t$, the RNN computes the hidden state as follows:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $W_{xh}$: weights for the input $x_t$\n",
        "* $W_{hh}$: recurrent weights for the previous hidden state $h_{t-1}$\n",
        "* $b_h$: bias term\n",
        "* $\\tanh$: non-linearity to bound the activations (can also be ReLU or others)\n",
        "\n",
        "> Note: The same weights $W_{xh}$ and $W_{hh}$ are **shared** across all time steps. This reduces the number of parameters and enforces consistency across sequence positions.\n",
        "\n",
        "\n",
        "### Producing Outputs\n",
        "\n",
        "If the RNN is used for sequence output tasks, each hidden state may be mapped to an output $y_t$ like this:\n",
        "\n",
        "$$\n",
        "y_t = \\text{softmax}(W_{hy} h_t + b_y)\n",
        "$$\n",
        "\n",
        "Where $W_{hy}$ and $b_y$ are parameters used to transform the hidden state into an output distribution. The **softmax** function ensures the output is a valid probability distribution(especially useful in classification problems)\n",
        "\n",
        "> Note for self: The difference between Softmax and Sigmoid functions is as follows:\n",
        "> - **Sigmoid** takes a single input (or the output of a single neuron) and maps it to a value between 0 and 1. This is primarily used in **binary classification**, where the output represents the probability of belonging to one of two classes.\n",
        "> - **Softmax** takes a vector of inputs (typically the raw outputs of the final layer's neurons in a multi-class setting) and converts it into a probability distribution. Each element of the output vector is between 0 and 1, and the **sum of all elements in the output vector is always 1**. This is essential for **multi-class classification**, as it gives the probability of the input belonging to each class, and these probabilities sum up to 1.\n",
        "\n",
        "\n",
        "## Feed Forward Pass\n",
        "\n",
        "In a standard feed-forward neural network like the ones used in previous architectures(CNN & MLP), each input is processed independently, passing through the network's layers in a single direction, from input to output, without any awareness of previous inputs. However, this structure is now not enough when dealing with sequential data, where the meaning or output at a given time depends heavily on what was fed in the network  before. To address this limitation, Recurrent Neural Networks (RNNs) introduce the concept of **temporal memory** through a mechanism that allows the network to maintain a hidden state across time steps.\n",
        "\n",
        "In the forward pass of an RNN, we start with an input sequence, say $[x_1, x_2, x_3, \\ldots, x_T]$, where each $x_t$ is a vector representing the input at time step $t$. Unlike in an MLP, where each input would be passed through the network separately, an RNN processes this sequence iteratively. At each time step, the network takes two pieces of information: the current input $x_t$, and the **hidden state** from the previous step, $h_{t-1}$. This hidden state acts as the network's memory, capturing information about all preceding inputs up to that point.\n",
        "\n",
        "Once the hidden state $h_t$ is computed, it can be transformed into an output $y_t$, depending on the task. In classification problems, this output is typically passed through a **softmax** layer to convert the raw scores into probabilities:\n",
        "\n",
        "$$\n",
        "y_t = \\text{softmax}(W_{hy} h_t + b_y)\n",
        "$$\n",
        "\n",
        "where $W_{hy}$ maps the hidden state to the output space, and $b_y$ is an output bias term. This output, $y_t$, can then be compared to the true label to compute a loss, which will later be used in backpropagation.\n",
        "\n",
        "To make this concrete, suppose we're training an RNN to predict the next character in a name. We might input the characters one at a time: first 'h', then 'a', then 'n'. Each character is converted into a numerical embedding (vector), and at each time step, the RNN computes a new hidden state based on the character and the previous hidden state. For example, at time $t=1$, the model receives 'h' as input and computes $h_1$. Then at $t=2$, it processes 'a' along with $h_1$, producing $h_2$, and so on. This sequential memory lets the network capture patterns like letter combinations or word structure that would otherwise be invisible to a feedforward model.\n",
        "\n",
        "What makes the RNN forward pass important is this chaining mechanism: each time step builds on the previous one, letting the model mimic “remembering” context. And since the weights are shared across steps, the model doesn't grow in complexity with longer sequences. However, this also introduces challenges like vanishing gradients, which we'll discuss when we reach backpropagation.\n",
        "\n",
        "##  Backpropagation Through Time (BPTT)\n",
        "\n",
        "Once an RNN has made its predictions through a **forward pass**, the next step is to check how accurate those predictions are and update the network's parameters to improve future predictions. This is done using **Backpropagation Through Time (BPTT)**, which extends the standard backpropagation algorithm used in feedforward neural networks explained before, this is done by accounting for the temporal structure of sequential data.\n",
        "\n",
        "To understand BPTT, it's useful to first recall the backpropagation process from our previous feed forward network. After computing the loss between the predicted and actual outputs, we calculated gradients of the loss function with respect to each weight in the network using the **chain rule** from calculus. These gradients tell us how sensitive the loss is to small changes in the weights, and we use this information to update the weights in the direction that minimizes the loss.\n",
        "\n",
        "In RNNs, this same idea applies, but with just a small twist: the same weights are **shared across time steps**. That is, the input weight matrix $W_{xh}$, the hidden-to-hidden recurrent matrix $W_{hh}$, and the output matrix $W_{hy}$ are used **repeatedly** at each time step of the sequence. Therefore, we must consider how the loss depends not only on the weights' influence at a single time step, but on their **cumulative influence across the entire sequence**.\n",
        "\n",
        "This is where the \"through time\" part comes in. During BPTT, we **unroll the RNN** across the sequence(see the figure above), essentially treating it like a deep feedforward network where each layer corresponds to a different time step. For a sequence of length $T$, this unrolling produces $T$ copies of the RNN cell, each with its own input $x_t$, hidden state $h_t$, and output $y_t$. Importantly, while each \"copy\" in this unrolled structure appears separate, they all share the **same set of weights**.\n",
        "\n",
        "After computing the loss $L$, which might be the sum of the individual losses at each time step (e.g., $L = \\sum_{t=1}^{T}\\mathcal{L}(y_t, \\hat{y}_t)$), we begin computing gradients backwards through this unrolled network. This means that at time step $t$, we compute the gradient of the loss with respect to the current hidden state $h_t$, as well as how this hidden state affects earlier hidden states through the recurrent connections. Since $h_t$ depends on $h_{t-1}$, which in turn depends on $h_{t-2}$, and so on, the error at each time step must be **backpropagated not only through the layers of the network but also back through time** to account for the influence of earlier states. For the visual, check the figure below:\n",
        "![An Image showing the back Propagation in RNN](https://raw.githubusercontent.com/mmuratarat/mmuratarat.github.io/master/_posts/images/BPTT.png)\n",
        "\n",
        "In the figure above,\n",
        "* $L$ represents The Loss at that time step,\n",
        "* $y$ represents the true label\n",
        "\n",
        "The Way the loss is propagated backwards through time is indicated in red.\n",
        "datasetdataset\n",
        "Mathematically, we compute the gradient of the total loss $L$ with respect to the shared weight matrices by summing up the contributions from each time step:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{xh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{xh}}, \\quad\n",
        "\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}}\n",
        "$$\n",
        "\n",
        "The gradient of the loss with respect to the hidden state $h_t$ includes contributions from the loss at time $t$ as well as from future time steps $t+1, t+2, \\dots$, since those future states also depend on $h_t$. This accumulation of gradients is why BPTT can become computationally expensive for long sequences, and it also explains why RNNs suffer from **vanishing or exploding gradients**. As errors are propagated backwards through many time steps, the repeated multiplication of derivatives can either shrink the gradient (making learning very slow) or cause it to grow uncontrollably (leading to instability or exploding grads). This is why deep RNNs often struggle with learning long-term dependencies(Will use LSTMs in future).\n",
        "\n",
        "In practice, to manage computational cost and instability, many implementations use **truncated BPTT**, where the backward pass only extends over a limited number of time steps (e.g., 20 or 30), rather than the entire sequence. This allows the network to learn short-to-medium-term dependencies without overwhelming the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## References\n",
        "[1. Lecture 10 | Recurrent Neural Networks. Stanford University](https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PLatU3hp4Hw2MKp0ylGWcS41f7gq1iF-ud&index=12)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eorx6RiC2NlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNAVxYbY2IAh",
        "outputId": "8f22775c-fc33-463d-91b8-d3043c33007b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST data...\n",
            "Data loaded and preprocessed.\n",
            "Model parameters initialized randomly.\n",
            "\n",
            "Starting RNN training...\n",
            "Epoch 1/10, Current Learning Rate: 0.001000\n",
            "Epoch 1, Loss: 0.7153\n",
            "Epoch 2/10, Current Learning Rate: 0.000980\n",
            "Epoch 2, Loss: 0.3222\n",
            "Epoch 3/10, Current Learning Rate: 0.000960\n",
            "Epoch 3, Loss: 0.2546\n",
            "Epoch 4/10, Current Learning Rate: 0.000941\n",
            "Epoch 4, Loss: 0.2069\n",
            "Epoch 5/10, Current Learning Rate: 0.000922\n",
            "Epoch 5, Loss: 0.1819\n",
            "Epoch 6/10, Current Learning Rate: 0.000904\n",
            "Epoch 6, Loss: 0.1670\n",
            "Epoch 7/10, Current Learning Rate: 0.000886\n",
            "Epoch 7, Loss: 0.1501\n",
            "Epoch 8/10, Current Learning Rate: 0.000868\n",
            "Epoch 8, Loss: 0.1361\n",
            "Epoch 9/10, Current Learning Rate: 0.000851\n",
            "Epoch 9, Loss: 0.1255\n",
            "Epoch 10/10, Current Learning Rate: 0.000834\n",
            "Epoch 10, Loss: 0.1084\n",
            "Model saved to simple_rnn_model.npz\n",
            "\n",
            "Evaluating RNN on test set...\n",
            "Test Accuracy: 94.84%\n",
            "\n",
            "Displaying a sample prediction from the test set (by index):\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math # For Xavier math manipulation..\n",
        "import os # For checking file existence and saving/loading\n",
        "\n",
        "\n",
        "IMAGE_DIM = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    will add docstrings later on...\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int,\n",
        "                 learning_rate: float = 0.001, clip_value: float = 5.0,\n",
        "                 dropout_rate: float = 0.0, loaded_params: dict = None):\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.clip_value = clip_value\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Adam optimizer parameters\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "        self.t = 0 # Time step for Adam bias correction\n",
        "\n",
        "        if loaded_params:\n",
        "            # Load pre-trained parameters- From File..\n",
        "            self.Wxh = loaded_params['Wxh']\n",
        "            self.Whh = loaded_params['Whh']\n",
        "            self.Why = loaded_params['Why']\n",
        "            self.bh = loaded_params['bh']\n",
        "            self.by = loaded_params['by']\n",
        "\n",
        "            # Load Adam optimizer states\n",
        "            self.mWxh, self.vWxh = loaded_params['mWxh'], loaded_params['vWxh']\n",
        "            self.mWhh, self.vWhh = loaded_params['mWhh'], loaded_params['vWhh']\n",
        "            self.mWhy, self.vWhy = loaded_params['mWhy'], loaded_params['vWhy']\n",
        "            self.mbh, self.vbh = loaded_params['mbh'], loaded_params['vbh']\n",
        "            self.mby, self.vby = loaded_params['mby'], loaded_params['mby']\n",
        "            self.t = loaded_params.get('t', 0) # Load Adam time step. default = 0\n",
        "            print(\"Model parameters and optimizer states loaded successfully.\")\n",
        "        else:\n",
        "            limit_wxh = math.sqrt(6 / (input_size + hidden_size))\n",
        "            self.Wxh = np.random.uniform(-limit_wxh, limit_wxh, (input_size, hidden_size))\n",
        "\n",
        "            limit_whh = math.sqrt(6 / (hidden_size + hidden_size))\n",
        "            self.Whh = np.random.uniform(-limit_whh, limit_whh, (hidden_size, hidden_size))\n",
        "\n",
        "            limit_why = math.sqrt(6 / (hidden_size + output_size))\n",
        "            self.Why = np.random.uniform(-limit_why, limit_why, (hidden_size, output_size))\n",
        "\n",
        "            self.bh = np.zeros((1, hidden_size))\n",
        "            self.by = np.zeros((1, output_size))\n",
        "\n",
        "            # Adam optimizer parameters (first and second moments of gradients)\n",
        "            self.mWxh, self.vWxh = np.zeros_like(self.Wxh), np.zeros_like(self.Wxh)\n",
        "            self.mWhh, self.vWhh = np.zeros_like(self.Whh), np.zeros_like(self.Whh)\n",
        "            self.mWhy, self.vWhy = np.zeros_like(self.Why), np.zeros_like(self.Why)\n",
        "            self.mbh, self.vbh = np.zeros_like(self.bh), np.zeros_like(self.bh)\n",
        "            self.mby, self.vby = np.zeros_like(self.by), np.zeros_like(self.by)\n",
        "            print(\"Model parameters initialized randomly.\")\n",
        "\n",
        "        self.is_training = True # to control dropout behavior\n",
        "\n",
        "    def tanh(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Applies the hyperbolic tan activation function.\"\"\"\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the derivative of the tanh function.\n",
        "        Derivative is 1 - tanh(z)^2.\n",
        "        \"\"\"\n",
        "        return 1 - x ** 2\n",
        "\n",
        "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Applies the softmax function to convert logits to probabilities.\n",
        "        Handles numerical stability by subtracting the max value.\n",
        "        (Similar to what we have done before..)\n",
        "        \"\"\"\n",
        "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the cross-entropy loss between predictions and\n",
        "        one-hot encoded targets.\n",
        "        Includes a small epsilon for numerical stability with log(0).\n",
        "        - Same implimentation as before.\n",
        "        \"\"\"\n",
        "        # Ensure prediction values are not exactly zero before taking log\n",
        "        prediction = np.clip(prediction, 1e-9, 1.0 - 1e-9)\n",
        "        return -np.sum(target * np.log(prediction)) / target.shape[0]\n",
        "\n",
        "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the RNN.\n",
        "        \"\"\"\n",
        "        # inputs shape: (batch_size, time_steps, input_size)\n",
        "        self.batch_size, self.time_steps, _ = inputs.shape\n",
        "        self.inputs = inputs # Store inputs for backprop pass\n",
        "\n",
        "        # Initialize hidden states list. hs[0] is h_0 (initial hidden state).\n",
        "        self.hs = [np.zeros((self.batch_size, self.hidden_size))]\n",
        "        self.dropout_masks = [] # Store dropout masks for backprop\n",
        "\n",
        "        for t in range(self.time_steps):\n",
        "            x_t = inputs[:, t, :]\n",
        "            h_prev = self.hs[-1]\n",
        "\n",
        "\n",
        "            h_t = self.tanh(np.dot(x_t, self.Wxh) + np.dot(h_prev, self.Whh) + self.bh)\n",
        "\n",
        "            if self.is_training and self.dropout_rate > 0:\n",
        "                dropout_mask = (np.random.rand(*h_t.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
        "                h_t *= dropout_mask\n",
        "                self.dropout_masks.append(dropout_mask)\n",
        "            else:\n",
        "                self.dropout_masks.append(np.ones_like(h_t))\n",
        "\n",
        "            self.hs.append(h_t)\n",
        "        self.y_logits = np.dot(self.hs[-1], self.Why) + self.by\n",
        "        self.y_pred = self.softmax(self.y_logits)\n",
        "        return self.y_pred\n",
        "\n",
        "    def backward(self, target: np.ndarray):\n",
        "        \"\"\"\n",
        "        Performs backpropagation through time (BPTT) and updates weights using Adam.\n",
        "\n",
        "        Args:\n",
        "            target (np.ndarray): One-hot encoded true labels, shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        # Increment Adam's time step counter\n",
        "        self.t += 1\n",
        "        dL_dy = self.y_pred - target\n",
        "        dWhy = np.dot(self.hs[-1].T, dL_dy)\n",
        "        dby = np.sum(dL_dy, axis=0, keepdims=True)\n",
        "\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dh_next = np.dot(dL_dy, self.Why.T)\n",
        "\n",
        "        # Backpropagate through time\n",
        "        for t in reversed(range(self.time_steps)):\n",
        "            h_t = self.hs[t+1]    # Current hidden state\n",
        "            h_prev = self.hs[t]   # Previous hidden state\n",
        "            x_t = self.inputs[:, t, :] # Input at current time step\n",
        "\n",
        "            dh = dh_next * self.tanh_derivative(h_t)\n",
        "            if self.is_training and self.dropout_rate > 0:\n",
        "                dh *= self.dropout_masks[t]\n",
        "            dWxh += np.dot(x_t.T, dh)\n",
        "            dWhh += np.dot(h_prev.T, dh)\n",
        "            dbh += np.sum(dh, axis=0, keepdims=True)\n",
        "            dh_next = np.dot(dh, self.Whh.T)\n",
        "\n",
        "        for grad in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(grad, -self.clip_value, self.clip_value, out=grad)\n",
        "\n",
        "        # Update function for Adam\n",
        "        def adam_update(param, m, v, grad):\n",
        "            m = self.beta1 * m + (1 - self.beta1) * grad\n",
        "            v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
        "            # Bias correction for first and second moments\n",
        "            m_hat = m / (1 - self.beta1 ** self.t)\n",
        "            v_hat = v / (1 - self.beta2 ** self.t)\n",
        "            param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            return param, m, v\n",
        "\n",
        "        self.Wxh, self.mWxh, self.vWxh = adam_update(self.Wxh, self.mWxh, self.vWxh, dWxh)\n",
        "        self.Whh, self.mWhh, self.vWhh = adam_update(self.Whh, self.mWhh, self.vWhh, dWhh)\n",
        "        self.Why, self.mWhy, self.vWhy = adam_update(self.Why, self.mWhy, self.vWhy, dWhy)\n",
        "        self.bh, self.mbh, self.vbh = adam_update(self.bh, self.mbh, self.vbh, dbh)\n",
        "        self.by, self.mby, self.vby = adam_update(self.by, self.mby, self.vby, dby)\n",
        "\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 10, batch_size: int = 64,\n",
        "              learning_rate_decay: float = 0.95):\n",
        "        \"\"\"\n",
        "        Trains the RNN model using the provided data.\n",
        "        \"\"\"\n",
        "        self.is_training = True\n",
        "        initial_learning_rate = self.learning_rate\n",
        "        num_samples = X.shape[0]\n",
        "        X_reshaped = X.reshape(-1, IMAGE_DIM, IMAGE_DIM)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            self.learning_rate = initial_learning_rate * (learning_rate_decay ** epoch)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Current Learning Rate: {self.learning_rate:.6f}\")\n",
        "            permutation = np.random.permutation(num_samples)\n",
        "            X_shuffled = X_reshaped[permutation]\n",
        "            y_shuffled = y[permutation]\n",
        "\n",
        "            num_batches = 0\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                batch_X = X_shuffled[i:i+batch_size]\n",
        "                batch_y = y_shuffled[i:i+batch_size]\n",
        "                if batch_X.shape[0] == 0:\n",
        "                    continue\n",
        "\n",
        "                y_pred = self.forward(batch_X)\n",
        "                loss = self.cross_entropy_loss(y_pred, batch_y)\n",
        "                total_loss += loss\n",
        "                num_batches += 1\n",
        "\n",
        "                self.backward(batch_y)\n",
        "\n",
        "            # Calculate average loss for the epoch\n",
        "            avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "        self.is_training = False # Set prediction mode after training\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Makes predictions on new input data.\n",
        "        \"\"\"\n",
        "        self.is_training = False\n",
        "        # Reshape input for forward pass if it's flattened (e.g., 784 pixels)\n",
        "        if X.ndim == 2 and X.shape[1] == IMAGE_DIM * IMAGE_DIM:\n",
        "            X_reshaped = X.reshape(-1, IMAGE_DIM, IMAGE_DIM)\n",
        "        elif X.ndim == 3 and X.shape[2] == IMAGE_DIM:\n",
        "            X_reshaped = X # Already in correct shape\n",
        "        else:\n",
        "            raise ValueError(f\"Input X has unexpected shape: {X.shape}. \"\n",
        "                             f\"Expected (num_samples, {IMAGE_DIM*IMAGE_DIM}) or (num_samples, {IMAGE_DIM}, {IMAGE_DIM}).\")\n",
        "\n",
        "        y_pred = self.forward(X_reshaped)\n",
        "        predicted_labels = np.argmax(y_pred, axis=1)\n",
        "        confidence = np.max(y_pred, axis=1)\n",
        "        return predicted_labels, confidence\n",
        "\n",
        "    def save_model(self, filepath: str):\n",
        "        \"\"\"\n",
        "        Saves the model's parameters (weights, biases, and Adam optimizer states) to a .npz file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): The path to save the model file (e.g., 'rnn_model.npz').\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'Wxh': self.Wxh, 'Whh': self.Whh, 'Why': self.Why,\n",
        "            'bh': self.bh, 'by': self.by,\n",
        "            'mWxh': self.mWxh, 'vWxh': self.vWxh,\n",
        "            'mWhh': self.mWhh, 'vWhh': self.vWhh,\n",
        "            'mWhy': self.mWhy, 'vWhy': self.vWhy,\n",
        "            'mbh': self.mbh, 'vbh': self.vbh,\n",
        "            'mby': self.mby, 'vby': self.mby,\n",
        "            't': self.t # Save Adam time step too..\n",
        "        }\n",
        "        np.savez(filepath, **params)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath: str, input_size: int, hidden_size: int, output_size: int,\n",
        "                   learning_rate: float = 0.001, clip_value: float = 5.0, dropout_rate: float = 0.0):\n",
        "        \"\"\"\n",
        "        Loads model parameters from a .npz file and initializes a SimpleRNN instance.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"Model file not found at {filepath}\")\n",
        "\n",
        "        loaded_params = np.load(filepath)\n",
        "        # Pass loaded_params to the constructor\n",
        "        return cls(input_size, hidden_size, output_size, learning_rate,\n",
        "                   clip_value, dropout_rate, loaded_params=dict(loaded_params))\n",
        "\n",
        "\n",
        "def one_hot_encode(labels: np.ndarray, num_classes: int = NUM_CLASSES) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    will add docstrings later on...\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "def display_single_prediction(rnn_model: SimpleRNN, image_data: np.ndarray = None,\n",
        "                              true_label: int = None, sample_index: int = None,\n",
        "                              X_dataset: np.ndarray = None, y_dataset: np.ndarray = None):\n",
        "    \"\"\"\n",
        "    Displays a single prediction from the test set.\n",
        "    will add docstrings later on...\n",
        "    \"\"\"\n",
        "    single_image = None\n",
        "    display_true_label = \"N/A\"\n",
        "\n",
        "    if image_data is not None:\n",
        "        if true_label is None:\n",
        "            print(\"Warning: true_label should be provided when image_data is used for display.\")\n",
        "        single_image = image_data\n",
        "        display_true_label = true_label if true_label is not None else \"N/A\"\n",
        "    elif sample_index is not None and X_dataset is not None and y_dataset is not None:\n",
        "        if not (0 <= sample_index < X_dataset.shape[0]):\n",
        "            print(f\"Error: Sample index {sample_index} is out of bounds for data with {X_dataset.shape[0]} samples.\")\n",
        "            return\n",
        "        single_image = X_dataset[sample_index]\n",
        "        display_true_label = y_dataset[sample_index]\n",
        "    else:\n",
        "        print(\"Error: Either 'image_data' with optional 'true_label' OR 'sample_index' with 'X_dataset' and 'y_dataset' must be provided.\")\n",
        "        return\n",
        "\n",
        "    # Ensure single_image is a numpy array before reshaping\n",
        "    if not isinstance(single_image, np.ndarray):\n",
        "        print(\"Error: Input image data is not a numpy array.\")\n",
        "        return\n",
        "\n",
        "    # Reshape the single image to the expected input format for prediction\n",
        "    # Assuming predict expects (batch_size, time_steps, input_size) or (batch_size, input_features)\n",
        "    # Based on the predict method, it handles flattened (num_samples, 784) or reshaped (num_samples, 28, 28)\n",
        "    single_image_flat = single_image.reshape(1, -1) # Reshape to (1, 784)\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_label, confidence = rnn_model.predict(single_image_flat)\n",
        "    predicted_label = predicted_label[0]\n",
        "    confidence = confidence[0]\n",
        "\n",
        "    # Reshape the image back to 28x28 for plots\n",
        "    image_2d = single_image.reshape(IMAGE_DIM, IMAGE_DIM)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image_2d, cmap='gray_r') # Use gray_r for white digits on black background\n",
        "    plt.title(f\"True: {display_true_label} | Predicted: {predicted_label}\\nConfidence: {(confidence:.4f)*100}%\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Loading MNIST data...\")\n",
        "try:\n",
        "    train_data = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "    test_data = pd.read_csv('/content/sample_data/mnist_test.csv', header=None)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: MNIST CSV files not found. Please ensure 'mnist_train_small.csv' and 'mnist_test.csv' are in '/content/sample_data/'.\")\n",
        "    print(\"You might need to download them or adjust the path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "X_train = train_data.iloc[:, 1:].values / 255.0\n",
        "y_train = one_hot_encode(train_data.iloc[:, 0].values, num_classes=NUM_CLASSES)\n",
        "\n",
        "X_test = test_data.iloc[:, 1:].values / 255.0\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "print(\"Data loaded and preprocessed.\")\n",
        "\n",
        "rnn = SimpleRNN(input_size=IMAGE_DIM, hidden_size=128, output_size=NUM_CLASSES,\n",
        "                learning_rate=0.001, clip_value=5.0, dropout_rate=0.0) # Added dropout\n",
        "\n",
        "print(\"\\nStarting RNN training...\")\n",
        "rnn.train(X_train, y_train, epochs=10, batch_size=64, learning_rate_decay=0.98)\n",
        "\n",
        "\n",
        "model_filepath = 'simple_rnn_model.npz'\n",
        "rnn.save_model(model_filepath)\n",
        "\n",
        "\n",
        "# print(f\"\\nAttempting to load model from {model_filepath}...\")\n",
        "# try:\n",
        "#     loaded_rnn = SimpleRNN.load_model(model_filepath,\n",
        "#                                       input_size=IMAGE_DIM, hidden_size=128, output_size=NUM_CLASSES,\n",
        "#                                       learning_rate=0.001, clip_value=5.0, dropout_rate=0.2)\n",
        "#     print(\"Model loaded successfully. Evaluating loaded model:\")\n",
        "#     predictions_loaded, _ = loaded_rnn.predict(X_test)\n",
        "#     accuracy_loaded = np.mean(predictions_loaded == y_test) * 100\n",
        "#     print(f\"Loaded Model Test Accuracy: {accuracy_loaded:.2f}%\")\n",
        "#     rnn = loaded_rnn # Use the loaded model for subsequent operations\n",
        "# except FileNotFoundError as e:\n",
        "#     print(e)\n",
        "#     print(\"Cannot load model, proceeding with freshly trained model.\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nEvaluating RNN on test set...\")\n",
        "predictions, _ = rnn.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nDisplaying a sample prediction from the test set (by index):\")\n",
        "display_single_prediction(rnn, sample_index=0, X_dataset=X_test, y_dataset=y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "AcumSDoDSuHW",
        "outputId": "3f0e9433-595e-4f75-8625-540db4bc034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Displaying a sample prediction from the test set (by index):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qy3pAjZCbbfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}